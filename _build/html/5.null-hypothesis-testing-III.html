
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>NHST III: Statistical Significance &#8212; Linear Models II: Statistical Inference</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/test.css?v=90b7ad94" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '5.null-hypothesis-testing-III';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="The Controversy of NHST" href="6.controversy.html" />
    <link rel="prev" title="NHST II: Test Statistics" href="4.null-hypothesis-testing-II.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="0.intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Linear Models II: Statistical Inference - Home"/>
    <img src="_static/logo.png" class="logo__image only-dark pst-js-only" alt="Linear Models II: Statistical Inference - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="0.intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="1.purpose-of-inference.html">The Purpose of Statistical Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="2.estimation-uncertainty.html">Uncertainty in Estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="3.null-hypothesis-testing-I.html">NHST I: The Null Hypothesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="4.null-hypothesis-testing-II.html">NHST II: Test Statistics</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">NHST III: Statistical Significance</a></li>
<li class="toctree-l1"><a class="reference internal" href="6.controversy.html">The Controversy of NHST</a></li>
<li class="toctree-l1"><a class="reference internal" href="summary.html">Summary</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/PCHN63101-Advanced-Data-Skills/Inference-Linear-Model" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/PCHN63101-Advanced-Data-Skills/Inference-Linear-Model/issues/new?title=Issue%20on%20page%20%2F5.null-hypothesis-testing-III.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/5.null-hypothesis-testing-III.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>NHST III: Statistical Significance</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-p-value-and-statistical-significance">The <span class="math notranslate nohighlight">\(p\)</span>-value and Statistical Significance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-the-p-value">Defining the <span class="math notranslate nohighlight">\(p\)</span>-value</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fisher-s-concept-of-the-p-value">Fisher’s Concept of the <span class="math notranslate nohighlight">\(p\)</span>-value</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neyman-pearson-nhst">Neyman-Pearson NHST</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modern-interpretation-of-nhst">Modern Interpretation of NHST</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fisher-vs-neyman-pearson">Fisher vs Neyman-Pearson</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modern-nhst">Modern NHST</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nhst-in-r">NHST in <code class="docutils literal notranslate"><span class="pre">R</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-implications">Practical Implications</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="nhst-iii-statistical-significance">
<h1>NHST III: Statistical Significance<a class="headerlink" href="#nhst-iii-statistical-significance" title="Link to this heading">#</a></h1>
<p>In the previous parts of this lesson, we established the core mechanics of NHST. During those discussions, we made multiple references to calculating <em>probabilities</em> in order to reach conclusions. In this part of the lesson, we will finally formalise how this is achieved. To briefly review, the process of NHST involves the following steps</p>
<ol class="arabic simple">
<li><p>Determine a null value for our parameter of interest. This does not have to be 0, but can be any value indicative of <em>no effect</em>.</p></li>
<li><p>Compare the null value to the actual value we have calculated and call this difference <span class="math notranslate nohighlight">\(\delta\)</span>.</p></li>
<li><p>Convert <span class="math notranslate nohighlight">\(\delta\)</span> into a <em>standardised</em> value by dividing by its standard error. The resultant value is called a <em>test statistic</em> and expresses our calculated difference in standard error units.</p></li>
<li><p>If we know the standard error, call the test statistic <span class="math notranslate nohighlight">\(z\)</span>. If we have estimated the standard error from the data, call the test statistic <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p>Finally, determine the <em>null distribution</em> of the test statistic. For a <span class="math notranslate nohighlight">\(z\)</span>-statistic, this is a standard normal distribution. For a <span class="math notranslate nohighlight">\(t\)</span>-statistic, this is a <span class="math notranslate nohighlight">\(t\)</span>-distribution whose width is governed by the <em>degrees of freedom</em>. This allows the distribution to flexibly accommodate how the precision of the estimated standard error influences the range of probable <span class="math notranslate nohighlight">\(t\)</span>-values.</p></li>
</ol>
<p>So, by this point, we have a test statistic that captures the deviation from the null in our dataset, and we known the distribution of the test-static when the null is true. Our very last step is to use both of these pieces of information to calculate the <em>probability</em> of our observed value of <span class="math notranslate nohighlight">\(t\)</span>, if the null hypothesis is true. This probability is perhaps one of the most controversial, misunderstood and misused aspects of classical statistical inference. This is the infamous <span class="math notranslate nohighlight">\(p\)</span>-value.</p>
<section id="the-p-value-and-statistical-significance">
<h2>The <span class="math notranslate nohighlight">\(p\)</span>-value and Statistical Significance<a class="headerlink" href="#the-p-value-and-statistical-significance" title="Link to this heading">#</a></h2>
<p>It is fairly likely (<span class="math notranslate nohighlight">\(p &lt; 0.05\)</span>) that you will have been exposed to the <span class="math notranslate nohighlight">\(p\)</span>-value already in your statistical education. If you come from a Psychology background, this is practically guaranteed as the history of scientific enquiry in the field of Psychology is largely the history of amassing <span class="math notranslate nohighlight">\(p\)</span>-values. As a method of trying to reach conclusions from a statistical model, the <span class="math notranslate nohighlight">\(p\)</span>-value is ubiquitous. This is largely because, on the face of it, the <span class="math notranslate nohighlight">\(p\)</span>-value provides an easily applicable and objective criteria for determining evidence against the null hypothesis. However, as we will come to see, the ability of the <span class="math notranslate nohighlight">\(p\)</span>-value to provide the information that researchers typically want is limited and arguably misleading. Despite the many issues with how <span class="math notranslate nohighlight">\(p\)</span>-values are used in practise, this humble little probability endures as the metric most commonly employed in Experimental Psychology to reach conclusions. As such, you <em>have</em> to understand this approach, even if you disagree with it. We will leave most of the criticisms to one side for the moment and just focus on making sure the definition of the <span class="math notranslate nohighlight">\(p\)</span>-value is clear. This is critical, as the <span class="math notranslate nohighlight">\(p\)</span>-value is often misunderstood by students and sometimes (rather worryingly) by researchers as well. As part of this, we will also examine the history of the <span class="math notranslate nohighlight">\(p\)</span>-value, and NHST in general, because this is illuminating in terms of understanding the role of the <span class="math notranslate nohighlight">\(p\)</span>-value and also, critically, why its modern useage can appear to be so logically unsatisfying.</p>
<section id="defining-the-p-value">
<h3>Defining the <span class="math notranslate nohighlight">\(p\)</span>-value<a class="headerlink" href="#defining-the-p-value" title="Link to this heading">#</a></h3>
<p>To begin with, it is useful to understand that the definition of the <span class="math notranslate nohighlight">\(p\)</span>-value comes directly from Fisher’s development of NHST. As we will explore further below, Fisher’s original conceptualisation of the <span class="math notranslate nohighlight">\(p\)</span>-value has been somewhat warped into our modern framework of NHST. So, for the moment, we will focus on Fisher’s original concept, before seeing what has happened over the years.</p>
<p>The basic definition of the <span class="math notranslate nohighlight">\(p\)</span>-value is the <em>probability of obtaining a test-statistic value as extreme, or more, assuming the null hypothsis is true</em>. Formally, we can write this as</p>
<div class="math notranslate nohighlight">
\[
p = P(t|\mathcal{H}_{0}),
\]</div>
<p>where <span class="math notranslate nohighlight">\(t\)</span> is our observed test-statistic value. How do we calculate this probability? Well, we know the distribution of <span class="math notranslate nohighlight">\(t\)</span> when the null hypothesis is true, as this is just the null <span class="math notranslate nohighlight">\(t\)</span>-distribution which is centred on 0 and has a width controlled by the degrees of freedom. So, if our hypothesised null value is <em>correct</em>, then we would expect the <span class="math notranslate nohighlight">\(t\)</span>-value calculated over many repeats to have a null <span class="math notranslate nohighlight">\(t\)</span>-distribution. This capture the degree of natural variation we would expect in the <span class="math notranslate nohighlight">\(t\)</span>-value across different samples, even when the null hypothesis is correct. If our calculated <span class="math notranslate nohighlight">\(t\)</span>-value appears <em>consistent</em> with this distribution, then it might be that the null is true. However, if our calculated <span class="math notranslate nohighlight">\(t\)</span>-value appears <em>inconsistent</em> with this distribution, then it might be that the null is incorrect.</p>
<p>A visualisation of the <span class="math notranslate nohighlight">\(p\)</span>-value is given in <a class="reference internal" href="#pval-fig"><span class="std std-numref">Fig. 2</span></a>. The distribution shown here represents the null distribution of the test statistic. The <span class="math notranslate nohighlight">\(p\)</span>-value is then the probability of the observed value of the test statistic, which corresponds to the area of the distribution <em>above</em> the observed value. This is important, because we are not calculating the probability of seeing <em>exactly</em> this calculated value under the null<a class="footnote-reference brackets" href="#probfoot" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>, instead it is the probability of this value <em>or larger</em>.</p>
<figure class="align-default" id="pval-fig">
<a class="reference internal image-reference" href="_images/pvalue.png"><img alt="_images/pvalue.png" src="_images/pvalue.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">Illustration of the p-value as the area under the null distribution of the test statistic above the calculated value.</span><a class="headerlink" href="#pval-fig" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>As an example, say we calculated <span class="math notranslate nohighlight">\(t = 1.2\)</span> and have <span class="math notranslate nohighlight">\(20\)</span> degrees of freedom. The <span class="math notranslate nohighlight">\(p\)</span>-value can then be derived in <code class="docutils literal notranslate"><span class="pre">R</span></code> using the <code class="docutils literal notranslate"><span class="pre">pt()</span></code> function (probability from the <span class="math notranslate nohighlight">\(t\)</span>-distribution)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1.2</span>
<span class="n">df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">20</span>
<span class="n">p</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">pt</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="n">t</span><span class="p">,</span><span class="w"> </span><span class="n">df</span><span class="o">=</span><span class="n">df</span><span class="p">,</span><span class="w"> </span><span class="n">lower.tail</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] 0.1220808
</pre></div>
</div>
</div>
</div>
<p>So, the probability of getting a <span class="math notranslate nohighlight">\(t\)</span>-value of <span class="math notranslate nohighlight">\(1.2\)</span> or larger when the null hypothesis is true is <span class="math notranslate nohighlight">\(p = 0.122\)</span>, or <span class="math notranslate nohighlight">\(12.2\%\)</span>. This is only based on the <em>upper-tail</em> of the distribution. If we wanted to test the probability of both <span class="math notranslate nohighlight">\(t &gt; 1.2\)</span> <em>and</em> <span class="math notranslate nohighlight">\(t &lt; -1.2\)</span>, we would add both tails together by multiplying the <span class="math notranslate nohighlight">\(p\)</span>-value by 2. This is typically done because we do not always have a strong <em>directional</em> hypothesis. In other words, we do not care whether <span class="math notranslate nohighlight">\(\delta\)</span> is positive or negative, only whether the difference is <em>large</em>. This is the difference between <em>one-tailed</em> and <em>two-tailed</em> tests, as shown below</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">one.tailed.p</span><span class="w">  </span><span class="o">&lt;-</span><span class="w">     </span><span class="nf">pt</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="n">t</span><span class="p">,</span><span class="w"> </span><span class="n">df</span><span class="o">=</span><span class="n">df</span><span class="p">,</span><span class="w"> </span><span class="n">lower.tail</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span>
<span class="n">two.tailed.p</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nf">pt</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="n">t</span><span class="p">,</span><span class="w"> </span><span class="n">df</span><span class="o">=</span><span class="n">df</span><span class="p">,</span><span class="w"> </span><span class="n">lower.tail</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span>

<span class="n">df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="s">&quot;One.tailed&quot;</span><span class="o">=</span><span class="n">one.tailed.p</span><span class="p">,</span>
<span class="w">                 </span><span class="s">&quot;Two.tailed&quot;</span><span class="o">=</span><span class="n">two.tailed.p</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">row.names</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> One.tailed Two.tailed
  0.1220808  0.2441616
</pre></div>
</div>
</div>
</div>
<p>Note that the majority of statistical software will show you two-tailed tests by default because strong directional hypotheses tend to be the <em>exception</em> rather than the rule and it is generally safer to report a <span class="math notranslate nohighlight">\(p\)</span>-value that is <em>too large</em> rather than one that is <em>too small</em>.</p>
</section>
<section id="fisher-s-concept-of-the-p-value">
<h3>Fisher’s Concept of the <span class="math notranslate nohighlight">\(p\)</span>-value<a class="headerlink" href="#fisher-s-concept-of-the-p-value" title="Link to this heading">#</a></h3>
<p>The next most obvious question is, once we have calculated the <span class="math notranslate nohighlight">\(p\)</span>-value, how do we interpret it? For Fisher, the <span class="math notranslate nohighlight">\(p\)</span>-value was a <em>continuous measure of evidence against the null</em>. The smaller the <span class="math notranslate nohighlight">\(p\)</span>-value, the greater the evidence <em>against</em> the null. Or, more precisely, the most <em>surprising</em> the data would be, if the null were true. In advocating this perspective of continuous evidence, Fisher provided some heuristics to aid interpretation:</p>
<blockquote class="epigraph">
<div><p>The value for which <span class="math notranslate nohighlight">\(p = 0.05\)</span>, or 1 in 20, is convenient to use in practice if we want to draw a line beyond which we say that the deviation is significant…</p>
<p class="attribution">—Fisher, <em>Statistical Methods for Research Workers</em> (1925)</p>
</div></blockquote>
<p>So Fisher suggested that we use <span class="math notranslate nohighlight">\(p &lt; 0.05\)</span> as a metric for determining <em>statistical significance</em>. However, the important point here (which is most readily forgotten in modern NHST) is that this is a <em>rough guideline</em> not some sort of <em>scientific truth</em>. Indeed, the full quote is much more illuminating</p>
<blockquote class="epigraph">
<div><p>The value for which <span class="math notranslate nohighlight">\(p = 0.05\)</span>, or 1 in 20, is convenient to use in practice if we want to draw a line beyond which we say that the deviation is significant, but it must not be forgotten that we shall often draw such a line when there is nothing there but chance.</p>
<p class="attribution">—Fisher, <em>Statistical Methods for Research Workers</em> (1925)</p>
</div></blockquote>
<p>So, Fisher viewed this criterion as a convenient heuristic, but fully acknowledged its limitations. A quote from later in his career is also illuminating of Fisher’s view</p>
<blockquote class="epigraph">
<div><p>The level of significance in such tests is a matter of convenience, and does not affect the logic of the procedure. The tests of significance…are not to be interpreted rigidly, but are meant to afford evidence, which can be taken into account, together with the rest of the evidence available</p>
<p class="attribution">—Fisher, <em>Statistical Methods and Scientific Inference</em> (1956)</p>
</div></blockquote>
<p>So, again, the criteria of <span class="math notranslate nohighlight">\(p &lt; 0.05\)</span> is not a rigid cut-off, but simply an interpretational device that should be used in conjunction with all other available evidence. It is interesting to think about this in light of criticisms of using a threshold of <span class="math notranslate nohighlight">\(p &lt; 0.05\)</span> for statistical inference. A more recent famous quote is</p>
<blockquote class="epigraph">
<div><p>Surely, God loves the 0.06 nearly as much as the 0.05?</p>
<p class="attribution">—Rosnow &amp; Rosenthal (1989)</p>
</div></blockquote>
<p>Indeed, Fisher would probably agree. As a continuous measure of evidence, a <span class="math notranslate nohighlight">\(p\)</span>-value of 0.06 <em>is</em> informative. Although not quite reaching our heuristic of <span class="math notranslate nohighlight">\(p &lt; 0.05\)</span>, this is still <em>very close</em> and suggests some modest evidence against the null. For Fisher, the actual value of <span class="math notranslate nohighlight">\(p\)</span> was <em>very important</em> for inference. Fisher <em>never</em> advocated the position of <em>rejecting</em> the null hypothesis when <span class="math notranslate nohighlight">\(p &lt; 0.05\)</span>. Indeed, Fisher strongly disagreed with this idea, which came from a separate development of NHST attributable to two statisticians called <a class="reference external" href="https://en.wikipedia.org/wiki/Jerzy_Neyman">Jerzy Neyman</a> and <a class="reference external" href="https://en.wikipedia.org/wiki/Egon_Pearson">Egon Pearson</a>.</p>
</section>
</section>
<section id="neyman-pearson-nhst">
<h2>Neyman-Pearson NHST<a class="headerlink" href="#neyman-pearson-nhst" title="Link to this heading">#</a></h2>
<p>In opposition to Fisher’s approach, Neyman-Pearson’s approach to NHST saw inference as an exercise in <em>decision making</em>. Moreso, they saw inference as the process of making decisions <em>under uncertainty</em>, emphasising the long-run control of error rates across repeated experiments. It is here that the concepts of the <em>alternative hypothesis</em>, <em>Type I/Type II errors</em>, <em>statistical power</em> and <em>rejection of the null</em> come from. Fisher, in his advocation of the <span class="math notranslate nohighlight">\(p\)</span>-value as a continuous metric of evidence, strongly disagreed with all these ideas. Neyman-Pearson NHST proceeds in several steps, which we will now briefly explore</p>
<p>First, we start by defining <em>two</em> competing hypotheses. The null hypothesis <span class="math notranslate nohighlight">\((\mathcal{H}_{0})\)</span> and the <em>alternative</em> hypothesis <span class="math notranslate nohighlight">\((\mathcal{H}_{1})\)</span>. The alternative requires the definition of an <em>effect size</em>, which is the magnitude of effect that we expect to see, if the alternative is true. We then define a test-statistic with a known distribution under the null for capturing the discrepancy between the data and the null hypothesis. This is the same procedure as Fisher’s approach and so does not need repeating.</p>
<p>Next, we need to consider the two different types of error that can occur when we make a decision about the null hypothesis:</p>
<ol class="arabic simple">
<li><p>We <em>reject</em> <span class="math notranslate nohighlight">\(\mathcal{H}_{0}\)</span> when it is <em>true</em></p>
<ul class="simple">
<li><p>This is a Type I error, also known as a <em>false-positive</em>, and is denoted <span class="math notranslate nohighlight">\(\alpha\)</span></p></li>
</ul>
</li>
<li><p>We <em>fail to reject</em> <span class="math notranslate nohighlight">\(\mathcal{H}_{0}\)</span> when <span class="math notranslate nohighlight">\(\mathcal{H}_{1}\)</span> is <em>true</em></p>
<ul class="simple">
<li><p>This is a Type II error, also known as a <em>false-negative</em>, and is denoted <span class="math notranslate nohighlight">\(\beta\)</span></p></li>
</ul>
</li>
</ol>
<p>The idea is that we want to set <span class="math notranslate nohighlight">\(\alpha\)</span> to an acceptable level to minimise false positives across repeat experiments. A typical value is <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>, which no doubt adds to the conflation of Fisher’s approach and Neyman-Pearson’s approach. We also want to <em>minimise</em> <span class="math notranslate nohighlight">\(\beta\)</span> or, equivalently, <em>maximise</em> <span class="math notranslate nohighlight">\(1 - \beta\)</span>. This is known as <em>statistical power</em> and can be interpreted as the probability of correctly rejecting <span class="math notranslate nohighlight">\(\mathcal{H}_{0}\)</span> when <span class="math notranslate nohighlight">\(\mathcal{H}_{1}\)</span> is true. This quantity depends upon the magnitude of the effect size, as well as the sample size and variance.</p>
<p>Based on our choice of <span class="math notranslate nohighlight">\(\alpha\)</span> and our test-statistic, we can then create a decision rule. This is achieved using the null distribution of the test-statistic. For instance, if we were using a <span class="math notranslate nohighlight">\(t\)</span>-statistic with <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span> and 20 degrees of freedom, the critical value is <span class="math notranslate nohighlight">\(t_{\text{crit}} = 1.725\)</span>. This can be calculated in <code class="docutils literal notranslate"><span class="pre">R</span></code> using</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0.05</span>
<span class="n">df</span><span class="w">     </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">20</span>
<span class="n">t.crit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">qt</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span><span class="w"> </span><span class="n">df</span><span class="o">=</span><span class="n">df</span><span class="p">,</span><span class="w"> </span><span class="n">lower.tail</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">t.crit</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] 1.724718
</pre></div>
</div>
</div>
</div>
<p>where the <code class="docutils literal notranslate"><span class="pre">qt()</span></code> function will calculate a <span class="math notranslate nohighlight">\(t\)</span>-value from a probability (whereas <code class="docutils literal notranslate"><span class="pre">pt()</span></code> calculated a probability from a <span class="math notranslate nohighlight">\(t\)</span>-value). If the null is true, we should only calculate a <span class="math notranslate nohighlight">\(t\)</span>-value <em>larger</em> than <span class="math notranslate nohighlight">\(1.725\)</span> around 5% of the time. Thus, over repeated experiments, if the null is true then we will only make a Type I error around 5% of the time. The Type I error rate has therefore been <em>controlled</em> at the given <span class="math notranslate nohighlight">\(\alpha\)</span>-level. Any value of <span class="math notranslate nohighlight">\(t &gt; 1.725\)</span> therefore falls within a <em>rejection-region</em>. In other words, a region of values of <span class="math notranslate nohighlight">\(t\)</span> which are so large that we will reject the null.</p>
<p>Finally, based on the value of <span class="math notranslate nohighlight">\(t\)</span> calculated from the data, we either <em>reject the null hypothesis</em> when <span class="math notranslate nohighlight">\(t &gt; t_{\text{crit}}\)</span>, or <em>fail to reject the null</em> when <span class="math notranslate nohighlight">\(t &lt; t_{\text{crit}}\)</span>. If we stick rigidly to these rejection regions then then probability of false-positives is controlled at the desired <span class="math notranslate nohighlight">\(\alpha\)</span>-level. Notice here that there is no contiuous assessment of the evidence against the null hypothesis, we simply <em>reject</em> or <em>accept</em> based on a strict threshold. Because of this, the <span class="math notranslate nohighlight">\(p\)</span>-value has <em>no role</em> in this procedure. The critical value of the test-statistic is determined <em>a priori</em> and we simply make a binary decision depending on what we calculate from the data. The precise control of the error rates requires a black-and-white decision. If our evidence shifts around and our decisions can be flexible, then we lose this control. For Neyman-Pearson, making a decision that controls long-term error <em>is the point</em> of statistical inference and a <span class="math notranslate nohighlight">\(p\)</span>-value is <em>unnecessary</em> for this purpose.</p>
</section>
<section id="modern-interpretation-of-nhst">
<h2>Modern Interpretation of NHST<a class="headerlink" href="#modern-interpretation-of-nhst" title="Link to this heading">#</a></h2>
<p>As we can see from the discussions above, the history of NHST is rather tangled and has often been misunderstood. This is largely because its modern incarnation is a hybrid of both Fisherian and Neyman-Pearsonian NHST. However, these are two <em>fundamentally different</em> statistical philosophies. Each framework were severly opposed by the other side, with major disagreements on the fundamental purpose of statistical inference and how evidence can be used. These two approaches were never meant to be combined. Indeed, both Fisher and Neyman-Pearson would probably agree that they <em>cannot</em> be combined in any meaningful way. And yet, modern NHST <em>has</em> merged these different ideas into a confused mish-mash that neither Fisher nor Neyman-Pearson would have endorsed.</p>
<section id="fisher-vs-neyman-pearson">
<h3>Fisher vs Neyman-Pearson<a class="headerlink" href="#fisher-vs-neyman-pearson" title="Link to this heading">#</a></h3>
<p>Fisher vehemently opposed Neyman-Pearson’s approach to NHST. As we saw above, Fisher believed in a continuous assessment of the evidence against the null, using heuristics to <em>guide</em> interpretation, rather than creating strict thresholds. Nature does not say “yes” or “no, rather it provides evidence that should be interpreted by science. Turning statistical inference into a simple binary decision makes the process robotic, automated and incapable of any nuance. For Fisher, this went against how nature actually operates. It is interesting to note that a common criticism of NHST is that it creates an unrealistic dichotomisiation of evidence<a class="footnote-reference brackets" href="#evidencefoot" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>, yet this is something that Fisher never endorsed. Fisher emphasised that a null hypothesis might never actually be true in reality. Instead, it is a simplifying assumption. The goal is therefore not to “accept” or “reject” it, but to see how well it can explain the data. In his own words, Fisher said:</p>
<blockquote class="epigraph">
<div><p>It is a mistake to speak of the null hypothesis as though it were a thing we are testing, or as though we were trying to accept or reject it.</p>
<p class="attribution">—Fisher, <em>Statistical Methods and Scientific Inference</em> (1956)</p>
</div></blockquote>
<p>In opposition, Neyman-Pearson were of the view that one cannot sit and ponder the meaning of a <span class="math notranslate nohighlight">\(p\)</span>-value forever. Eventually, some decision must be reached</p>
<ul class="simple">
<li><p>“Is this treatment effective enough to consider offering it to patients?”</p></li>
<li><p>“Is the quality control of this product good enough to enter production?”</p></li>
<li><p>“Are the side-effects of this drug severe enough that the medication should be banned?”</p></li>
</ul>
<p>In all these cases a clear decision must be made. For this to happen, we need rules for reaching decisions in the face of uncertainty that <em>minimise the probability of severe mistakes</em>. Indeed, Neyman was very critical of the seemingly subjective way a <span class="math notranslate nohighlight">\(p\)</span>-value could be used to reason about a hypothesis, particularly when a <span class="math notranslate nohighlight">\(p\)</span>-value is a statement about a <em>single set of data</em></p>
<blockquote class="epigraph">
<div><p>The test of a hypothesis is a rule which tells us whether to reject or not to reject a given hypothesis in the light of observed data. It is not a method of reasoning which we can apply at will to interpret a particular set of data.</p>
<p class="attribution">—Neyman, <em>“Inductive Behavior” as a Basic Concept of Philosophy of Science</em> (1957)</p>
</div></blockquote>
<p>Because of this, Neyman-Pearson fundamentally rejected the <span class="math notranslate nohighlight">\(p\)</span>-value as having any utility in their approach to inference. Because a <span class="math notranslate nohighlight">\(p\)</span>-value is <em>data dependant</em> it goes against the Neyman-Pearson philosophy of defining an error-control procedure that is divorced from the data itself. If we tie the <span class="math notranslate nohighlight">\(p\)</span>-value to a pre-specified <span class="math notranslate nohighlight">\(\alpha\)</span> (such as <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>), then it can be used as a decision rule, but the <em>actual value of p does not matter</em>. At which point, we may as well just use the critical value of the test statistic. The <span class="math notranslate nohighlight">\(p\)</span>-value adds nothing to this procedure. Neyman-Pearson would likely claim that the <span class="math notranslate nohighlight">\(p\)</span>-value is just a <em>post-hoc description of how surprising the data is</em>. To them, this is just a description of the data, not <em>inference</em>. The only reason to calculate a <span class="math notranslate nohighlight">\(p\)</span>-value is to consider how compatible our data is with the null hypothesis. But Neyman-Pearson were not interested in this. For them, statistical inference is simply a yes/no question with no grey areas.</p>
</section>
<section id="modern-nhst">
<h3>Modern NHST<a class="headerlink" href="#modern-nhst" title="Link to this heading">#</a></h3>
<p>In its most modern incarnation, NHST involves <em>both</em> the calculation of <span class="math notranslate nohighlight">\(p\)</span>-values <em>and</em> the use of stringent cut-offs that result in either <em>rejecting</em> or <em>failing to reject</em> the null. We speak of <em>statistical power</em>, <em>alternative hypotheses</em> and <span class="math notranslate nohighlight">\(\alpha\)</span><em>-levels</em>, yet our statistic software prints <span class="math notranslate nohighlight">\(p\)</span>-values rather than binary decisions. It is hopefully clear that this modern use of NHST would have horrified Fisher and Neyman-Pearson in equal measure.</p>
<p>Fisher would hate to see his <span class="math notranslate nohighlight">\(p\)</span>-values being used as a decision making tool for rejecting hypotheses, rather than as a measure of evidence. Neyman-Pearson would hate to see statistical software reporting <span class="math notranslate nohighlight">\(p\)</span>-values, rather than simply stating whether a test passed the critical threshold for rejection. Our modern approach to NHST bleeds decision making and thresholds into Fisher’s continuous measures of evidence, and bleeds continuous measures of evidence and the term “significance” into Neyman-Pearson’s strict decision making approach. This has been called the <em>hybrid logic</em> of modern NHST by <a class="reference external" href="https://media.pluto.psy.uconn.edu/Gigerenzer%20superego%20ego%20id.pdf">Gigerenzer (1993)</a>, who stated</p>
<blockquote class="epigraph">
<div><p>The Freudian metaphor suggests that the resulting conceptual confusion in the minds of researchers, editors, and textbook writers is not due to limited intelligence. The metaphor brings the anxiety and guilt, the compulsive and ritualistic behavior, and the dogmatic blindness associated with the hybrid logic into the foreground. It is as if the raging personal and intellectual conflicts between Fisher and Neyman and Pearson, and between these frequentists and the Bayesians were projected into an “intrapsychic” conflict in the minds of researchers. And the attempts of textbook writers to solve this conflict by denying it have produced remarkable emotional, behavioral, and cognitive distortions.</p>
<p class="attribution">—Gerd Gigerenzer, <em>The Superego, the Ego, and the Id in Statistical Reasoning</em> (1993)</p>
</div></blockquote>
<p>To see more clearly why the modern mish-mash of these ideas is philosophically inconsistent, consider the situation where we calculate <span class="math notranslate nohighlight">\(p = 0.001\)</span>:</p>
<ul class="simple">
<li><p>If we simply <em>reject</em> the null because <span class="math notranslate nohighlight">\(p &lt; 0.05\)</span> then we are doing Neyman-Pearson NHST but using Fisher’s <span class="math notranslate nohighlight">\(p\)</span>-values to implement it. There is nothing fundamentally wrong with this, as long as we are only using the <span class="math notranslate nohighlight">\(p\)</span>-value as a <em>binary indicator</em>. However, this begs the question what the point of the <span class="math notranslate nohighlight">\(p\)</span>-value is? We could reach the same conclusion using the critical value of the test-statistic, which is how Neyman-Pearson designed their approach. By looking at the <span class="math notranslate nohighlight">\(p\)</span>-value, the temptation is there to interpret its magnitude.</p></li>
<li><p>If we interpret this as strong evidence aginst the null, because the <span class="math notranslate nohighlight">\(p\)</span>-value is quite small, then we are doing Fisher’s NHST. We may use a heuristic of <span class="math notranslate nohighlight">\(p &lt; 0.05\)</span> as some marker of “significance”, but this is not a binary rule to accept or reject anything. We simply say that the data is quite surprising, if the null hypothesis were true. What we do <em>not</em> do is reject the null hypothesis on this basis, because it still remains possible that we have witnessed a rare event. Also, the null may never <em>actually</em> be true, so all we can really say is that these data seem incompatible with this simplifying device.</p></li>
<li><p>If we squash these two approaches together by rejecting the null hypothesis because <span class="math notranslate nohighlight">\(p &lt; 0.05\)</span>, but also (whether explicitly or implicitly) take the magnitude of the <span class="math notranslate nohighlight">\(p\)</span>-value as stronger evidence than another <span class="math notranslate nohighlight">\(p\)</span>-value in our output table (say <span class="math notranslate nohighlight">\(p = 0.048\)</span>), then we are doing the modern incoherent muddling of these two approaches.</p></li>
</ul>
<p>Although it <em>feels</em> like we should be able to combine these methods, this causes problems. For instance, it feels like we should be able to pre-specify a level of error-control, reject the null based on this level and then use the magnitude of the <span class="math notranslate nohighlight">\(p\)</span>-value to indicate our confidence in that deicison. However, what happens quite often is that researchers will treat values <em>close</em> to <span class="math notranslate nohighlight">\(p = 0.05\)</span> as <em>marginally significant</em>, or move their criteria for significance based on the calculated <span class="math notranslate nohighlight">\(p\)</span>-values. If we start moving the goalposts based on the <span class="math notranslate nohighlight">\(p\)</span>-values, then the whole principle of fixing error rates before collecting the data goes out the window. Furthermore, treating <span class="math notranslate nohighlight">\(p\)</span>-values as both a binary indicator <em>and</em> a continuous measure of evidence is inconsistent. For instance, consider calculating <span class="math notranslate nohighlight">\(p = 0.06\)</span>. Neyman-Pearson would say that you have <em>failed to reject the null</em>. It does not matter if <span class="math notranslate nohighlight">\(p = 0.06\)</span> or <span class="math notranslate nohighlight">\(p = 0.6\)</span>, the decision rule is final. Fisher would say that the data are somewhat surpring under the null. Although the value is not below our heuristic of <span class="math notranslate nohighlight">\(p &lt; 0.05\)</span>, it is suggestive of moderate evidence <em>against the null</em>. This may imply that more data is needed to fully understand what is going on, but there is certainly a hint that the null may not be true. So we reach two different conclusions depending on how we treat the <span class="math notranslate nohighlight">\(p\)</span>-value.</p>
<figure class="align-left">
<a class="reference internal image-reference" href="_images/pvaluepumpkin.jpg"><img alt="_images/pvaluepumpkin.jpg" src="_images/pvaluepumpkin.jpg" style="width: 320.0px; height: 288.0px;" /></a>
</figure>
<p>Anyone who has used NHST for long enough knows the pains of grappling with a <span class="math notranslate nohighlight">\(p\)</span>-value that is <em>nearly significant</em>. In these situations, the inherent conflict between a strict decision rule and a continuous measure of evidence becomes clear. We see a value that tells us that we cannot reject the null, yet the value is so close to the threshold that it feels as if it should <em>mean something</em>. This is especially true when we get a value like <span class="math notranslate nohighlight">\(p = 0.051\)</span> versus <span class="math notranslate nohighlight">\(p = 0.96\)</span>. In these situations, we want to be Fisher but have been taught to be Neyman-Pearson. We are trying to adhere to a yes/no decision-making framework, yet have been presented with a continuous metric of evidence. When this happens, researchers often try to find a way out by claiming “marginal significance”, which is another way of saying “yes, we know we have failed to reject the null here, but it is really close!” So is this meaningful or not? This use of NHST claims that a result with <span class="math notranslate nohighlight">\(p = 0.051\)</span> is simultaneously <em>meaningful</em> and <em>not meaningful</em>. We are trying to adhere to a strict threshold that maintains long-run error rates, but also move our goalposts around in terms of what we want to conclude based on how close the <span class="math notranslate nohighlight">\(p\)</span>-value is to that threshold. Logically, we cannot be doing both. We cannot both reject a result to maintain a long-term error rate <em>and</em> include it in our discussion because it is really close to the threshold. Given this, it is not really suprising that when faced with such a logical conflict, unscrupulous researchers will find means to <em>nudge</em> a <span class="math notranslate nohighlight">\(p\)</span>-value under the threshold and resolve their dilemma.</p>
<p>In reality, if we really want to adhere to the Neyman-Pearson framework, we should remove <span class="math notranslate nohighlight">\(p\)</span>-value columns from software. Instead, software should simply state whether to “reject” or “fail to reject” a result based on the critical value. No more information is given because no more information is needed and the conflict above is resolved. Alternatively, if we want to adhere to Fisher’s framework, we keep <span class="math notranslate nohighlight">\(p\)</span>-values but remove strict decision rules, as well as concepts of error rates and power. We simply see the <span class="math notranslate nohighlight">\(p\)</span>-value as a continuous metric of evidence and interpret it with certain heuristics in mind. Again, this resolves the conflict. What we cannot really do is have all these elements and remain logically consistent…and yet, here we are.</p>
</section>
</section>
<section id="nhst-in-r">
<h2>NHST in <code class="docutils literal notranslate"><span class="pre">R</span></code><a class="headerlink" href="#nhst-in-r" title="Link to this heading">#</a></h2>
<p>Despite the philosophical issues highlighted above, NHST remains a core element of modern statistical inference. We can muse about why this is from a historical perspective and complain about its inconsistencies, but that does not stop it from being used. As such, we need to at least see how NHST is implemented within <code class="docutils literal notranslate"><span class="pre">R</span></code>. First, let us look again at the output table that results from the multiple regression model <code class="docutils literal notranslate"><span class="pre">mpg</span> <span class="pre">~</span> <span class="pre">wt</span> <span class="pre">+</span> <span class="pre">hp</span> <span class="pre">+</span> <span class="pre">disp</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">data</span><span class="p">(</span><span class="n">mtcars</span><span class="p">)</span>
<span class="n">mod</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">mpg</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">wt</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">hp</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">disp</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">mtcars</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = mpg ~ wt + hp + disp, data = mtcars)

Residuals:
   Min     1Q Median     3Q    Max 
-3.891 -1.640 -0.172  1.061  5.861 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 37.105505   2.110815  17.579  &lt; 2e-16 ***
wt          -3.800891   1.066191  -3.565  0.00133 ** 
hp          -0.031157   0.011436  -2.724  0.01097 *  
disp        -0.000937   0.010350  -0.091  0.92851    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.639 on 28 degrees of freedom
Multiple R-squared:  0.8268,	Adjusted R-squared:  0.8083 
F-statistic: 44.57 on 3 and 28 DF,  p-value: 8.65e-11
</pre></div>
</div>
</div>
</div>
<p>Here we can see that each parameter estimate is listed, alongside the standard error estimate, the <span class="math notranslate nohighlight">\(t\)</span>-statistic and the associated two-tailed <span class="math notranslate nohighlight">\(p\)</span>-value. Each <span class="math notranslate nohighlight">\(t\)</span>-statistic is constructed by dividing the estimate by the standard error. As such, each test is an implicit comparison with a proposed population parameter of 0. This is a reasonable null for each slope, as it captures the idea of <em>no relationship</em> in the population. For the intercept, the utility of testing for a regression through the origin can be somewhat questionable and is dependent upon context.</p>
<p>To take a single example, consider the <code class="docutils literal notranslate"><span class="pre">wt</span></code> predictor. The estimates are <span class="math notranslate nohighlight">\(\hat{\beta}_{1} = -3.80\)</span> and <span class="math notranslate nohighlight">\(\text{SE}\left(\hat{\beta}_{1}\right) = 1.066\)</span>. The <span class="math notranslate nohighlight">\(t\)</span>-statistic is then</p>
<div class="math notranslate nohighlight">
\[
t = \frac{\hat{\beta}_{1} - 0}{\text{SE}\left(\hat{\beta}_{1}\right)} = \frac{-3.80}{1.066} = -3.565.
\]</div>
<p>This tells us that the difference between our estimated slope and 0 is -3.80 in the original units of MPG, which is equivalent to 3.565 standard errors. Because the estimated slope is <em>negative</em> the <span class="math notranslate nohighlight">\(t\)</span>-statistic is <em>also</em> negative. So this is 3.565 standard errors <em>below</em> 0. The associated <span class="math notranslate nohighlight">\(p\)</span>-value is <span class="math notranslate nohighlight">\(0.0013\)</span>, which we can also calculate manually in <code class="docutils literal notranslate"><span class="pre">R</span></code> using</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">summary</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[</span><span class="m">2</span><span class="p">,</span><span class="s">&quot;t value&quot;</span><span class="p">]</span><span class="w"> </span><span class="c1"># t-statsitic for wt</span>
<span class="n">df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mod</span><span class="o">$</span><span class="n">df.residual</span>
<span class="n">p</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nf">pt</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="nf">abs</span><span class="p">(</span><span class="n">t</span><span class="p">),</span><span class="w"> </span><span class="n">df</span><span class="o">=</span><span class="n">df</span><span class="p">,</span><span class="w"> </span><span class="n">lower.tail</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> </span><span class="c1"># absolute value of t for upper-tail</span>
<span class="nf">print</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] 0.001330991
</pre></div>
</div>
</div>
</div>
<p>This indicates that, if this slope were 0 in the population, the probability of calculating a <span class="math notranslate nohighlight">\(t\)</span>-statistic of -3.565 is about 0.13% (around a <em>tenth</em> of 1%, or 1 out of 1,000). Fisher would say that this value is significant because <span class="math notranslate nohighlight">\(p &lt; 0.05\)</span>, but would also emphasise that such a small <span class="math notranslate nohighlight">\(p\)</span>-value suggests that are data are <em>very inconsistent</em> with a null slope of 0 in the population. If we are using <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>, Neyman-Pearson would tell us to <em>reject</em> the null hypothesis that the slope is 0. In doing so, we would ignore the magnitude of the <span class="math notranslate nohighlight">\(p\)</span>-value because only the decision rule matters. As we can see, <code class="docutils literal notranslate"><span class="pre">R</span></code> tells us both. We have the <span class="math notranslate nohighlight">\(p\)</span>-values listed from Fisher, with stars next to them to indicate the Neyman-Pearson decision rule for different values of <span class="math notranslate nohighlight">\(\alpha\)</span> (as listed under <code class="docutils literal notranslate"><span class="pre">Signif.</span> <span class="pre">codes</span></code>).</p>
<section id="practical-implications">
<h3>Practical Implications<a class="headerlink" href="#practical-implications" title="Link to this heading">#</a></h3>
<p>In this example, the <em>practical</em> implication of adhering to either The Fisher or Neyman-Pearson approach is minimal. For both <code class="docutils literal notranslate"><span class="pre">wt</span></code> and <code class="docutils literal notranslate"><span class="pre">hp</span></code>, Fisher would say there was strong evidence against the null (though moreso for <code class="docutils literal notranslate"><span class="pre">wt</span></code> than <code class="docutils literal notranslate"><span class="pre">hp</span></code>) and Neyman-Pearson would tell us to reject the null for both tests. For <code class="docutils literal notranslate"><span class="pre">disp</span></code>, Fisher would say that there was very strong evidence <em>in favour</em> of the null or, more precisely, that the data are very <em>compatible</em> or <em>well-explained</em> by the null. Similarly, Neyman-Pearson would tell us that we have failed to reject the null. So, although these two concepts are philosophically at-odds, their practical application in this setting has led to similar conclusions. Where the issues appear are not in the extremes, but around the margins of the threshold. In other words, when we get <span class="math notranslate nohighlight">\(p = 0.049\)</span>, <span class="math notranslate nohighlight">\(p = 0.051\)</span> or (heaven help us) <span class="math notranslate nohighlight">\(p = 0.050\)</span>. We already discussed how the problem of induction has largely been side-stepped by science on the grounds of <em>pragmatism</em> and here we see the same phenomena occurring. Because <em>practically</em> the conclusions are very similar a lot of the time, we are happy to push forward mashing these approaches together. However, this conflict does get in the way sometimes and it feels rather unsatisfying to use a method with principles that do not make sense when examined closely, even if we can justify it pragmatically. We have already entered into the world of induction by appealing to pragmatism, and here we are again having to resolve another logical problem by appealing to pragmatism. <a class="reference external" href="https://en.wikipedia.org/wiki/Turtles_all_the_way_down">It is pragmatism all the way down…</a>.</p>
<aside class="topic">
<p class="topic-title">What do you now know?</p>
<p>In this section, we have explored the final part of NHST in terms of the concept of <span class="math notranslate nohighlight">\(p\)</span>-values, as well as different perspectives on hypothesis testing. After reading this section, you should have a good sense of:</p>
<ul class="simple">
<li><p>The definition of the <span class="math notranslate nohighlight">\(p\)</span>-value as the probability of a test-statistic as larger, or larger, than the one we have calculated, assuming the null hypothesis is true.</p></li>
<li><p>The difference between <em>one-tailed</em> and <em>two-tailed</em> <span class="math notranslate nohighlight">\(p\)</span>-values.</p></li>
<li><p>Fisher’s concept of the <span class="math notranslate nohighlight">\(p\)</span>-value as a continuous metric of evidence for the compatibility of the data with the null.</p></li>
<li><p>The idea that <span class="math notranslate nohighlight">\(p &lt; 0.05\)</span> was original a <em>heuristic</em> for interpretation, rather than a fixed threshold.</p></li>
<li><p>The conflicting view on hypothesis testing introduced by Neyman-Pearson, who saw statistical inference as a form of decision-making under uncertainty.</p></li>
<li><p>The fact that rejecting the null, concepts of Type I and Type II errors, and statistical power are all part of Neyman-Pearson’s framework that was <em>not</em> endorsed by Fisher.</p></li>
<li><p>The fact that modern NHST is a somewhat illogical hybrid of these two approaches.</p></li>
<li><p>The idea that justification for this hybrid approach can only really be made on pragmatic grounds.</p></li>
</ul>
</aside>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="probfoot" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Technically, the probability of an exact value (call it <span class="math notranslate nohighlight">\(x^{\ast}\)</span>) from a continuous probability distribution is 0. This is not to say it is <em>impossible</em>, rather it cannot be calculated because a single point has no area under the distribution curve. It has a height, given by the probability density at that point, but its width is 0 because it stretches from <span class="math notranslate nohighlight">\(x^{\ast} - x^{\ast} = 0\)</span>.</p>
</aside>
<aside class="footnote brackets" id="evidencefoot" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>This is also true of alternative methods that use cut-off values, even if they are <em>supposed</em> to guide interpretation rather than being <em>rules</em>. For instance, using “small”, “medium” or “large” to categorise effect sizes serves only to <em>trichotomise</em> evidence. Similarly, more recent applications of Bayes Factors as an alternative to NHST suffer from the same issue of catgorising results into “weak evidence”, “moderate evidence”, “strong evidence” etc. In all cases, the danger comes from an “interpretational device” turning into blind application of a rule, which is exactly what happened to Fisher’s heuristic of <span class="math notranslate nohighlight">\(p=0.05\)</span>.</p>
</aside>
</aside>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            name: "ir",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="4.null-hypothesis-testing-II.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">NHST II: Test Statistics</p>
      </div>
    </a>
    <a class="right-next"
       href="6.controversy.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">The Controversy of NHST</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-p-value-and-statistical-significance">The <span class="math notranslate nohighlight">\(p\)</span>-value and Statistical Significance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-the-p-value">Defining the <span class="math notranslate nohighlight">\(p\)</span>-value</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fisher-s-concept-of-the-p-value">Fisher’s Concept of the <span class="math notranslate nohighlight">\(p\)</span>-value</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neyman-pearson-nhst">Neyman-Pearson NHST</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modern-interpretation-of-nhst">Modern Interpretation of NHST</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fisher-vs-neyman-pearson">Fisher vs Neyman-Pearson</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modern-nhst">Modern NHST</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nhst-in-r">NHST in <code class="docutils literal notranslate"><span class="pre">R</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-implications">Practical Implications</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dr George Farmer & Dr Martyn McFarquhar
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2026.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>