{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6b3fd6-1753-45e7-a920-fccf68cbdcac",
   "metadata": {},
   "source": [
    "# Omnibus Tests and Model Comparisons\n",
    "In the previous section, we saw how to use dummy variables with multiple categorical levels and then specify a subsequent regression model that fits group means and mean differeces. Although we called this a One-way ANOVA model, it probably does not look much like an ANOVA to you yet. In this section, we will complete the picture by discussing the nature of the ANOVA omnibus tests and showing how these are effectively *model comparisons*. The familiar ANOVA table is simply a way of displaying the results of several model comparisons. We will show how to generate this within `R` and also show how the values within this table relate to comparing different regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599f0b40",
   "metadata": {},
   "source": [
    "## ANOVA as a Model Comparison Procedure\n",
    "In order to understand the ANOVA within the framework of linear models, we need to demonstrate how the ANOVA results are simply the outcomes from comparing different linear models in a specific way. To begin, we need to consider the logic of an *omnibus test*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e757115a",
   "metadata": {},
   "source": [
    "\n",
    "### Omnibus Tests\n",
    "An omnibus test is a test that contains *multiple* comparisons between means. In comparison to a procedure such as a $t$-test, which only compares *two* means, an omnibus test can compare *multiple* means. In our example of a One-way ANOVA where $k = 3$, the omnibus null hypothesis is\n",
    "\n",
    "$$\n",
    "\\mathcal{H}_{0} : \\mu_{1} = \\mu_{2} = \\mu_{3}.\n",
    "$$\n",
    "\n",
    "In other words, the null is that *all* the means are identical. Or, to put it another way, that all the mean differences are 0. An omnibus test of this hypothesis is able to simultaneously compare *all possible mean differences*. Within the NHST framework, a significant omnibus effect suggests that *at least one* of all possible mean differences is significant. Traditionally, we would then drill-down to see which of the differences is driving the omnibus effect[^omnibus-foot]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36146853",
   "metadata": {},
   "source": [
    "```{admonition} What is the point of an omnibus test?\n",
    ":class: tip, dropdown\n",
    "At this point, you may well ask: what is the point of an omnibus test? If we just end up drilling-down to work out which differences are significant, why not just start there? Why even both with the omnibus test? \n",
    "\n",
    "The traditional argument is one of *error control* in relation to the problem of *multiple testing* or *multiple comparisons*. If our categorical variable has $k$ levels, then there are $m = \\frac{k(k-1)}{2}$ comparisons we can make between the levels. If our desired error level is given by $\\alpha$, then across all tests the probability of *at least one* being significant is $1 - (1 - \\alpha)^{m}$. This is known as the *familywise error rate* (FWER). So, notice that whenever $m > 1$ our desired error-rate will be scaled by the number of tests and therefore get *bigger*. \n",
    "\n",
    "As an example, if we take $k = 5$, $m = 10$ and $\\alpha = 0.05$, the FWER is \n",
    "\n",
    "$$\n",
    "\\text{FWER} = 1 - (1 - \\alpha)^{m} = 1 - 0.95^{10} = 0.40.\n",
    "$$\n",
    "\n",
    "This means there is now a 40% chance of a significant effect under the null, rather than a 5% chance. This is the *multiple testing* problem. The more levels our categorical predictor has, the worse this gets. This is where an omnibus test helps because, irrespective of the value of $k$, the FWER stays at the desired $\\alpha$ because it is only a *single test*. So, we can have a factor with many levels and can assess all the differerences simultaneously using a single test that does not inflate the FWER.\n",
    "\n",
    "It is worth noting as well that multiple testing is really a problem of NHST and frequentist statistics. Because of these approaches, there is a sense that traditional statistical inference actively *punishes* researchers for wanting to investigate and explore their data. This is not universal of all statistical methods. For instance, Bayesian statistics allows as many comparisons as you want without disturbing the probabilistic structure of the conclusions. So just know that ominbus tests and the multiple comparisons problem are not *universal* in statistics, rather they are a consequence of one particular school of thought and one particular definition of probability.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfbe13e",
   "metadata": {},
   "source": [
    "### Omnibus Tests From Model Comparisons\n",
    "So, how do we generate omnibus tests from model comparisons? To see how this works, consider that the omnibus null hypothesis above actually *implies* a specific model. If it were true that all the group means are identical, then the grouping variable is effectively meaningless. It has simply chopped the data up randomly into 3 groups. Assuming that the population means of the groups are all the *same* implies that all the data are drawn from the *same population distibution*. Thus, rather than assuming\n",
    "\n",
    "$$\n",
    "y_{ij} \\sim \\mathcal{N}\\left(\\mu_{j},\\sigma^{2}\\right)\n",
    "$$\n",
    "\n",
    "we are assuming\n",
    "\n",
    "$$\n",
    "y_{i} \\sim \\mathcal{N}\\left(\\mu,\\sigma^{2}\\right).\n",
    "$$\n",
    "\n",
    "Here we have *two different models*. One that just assumes a single mean for all the data and one that assumes *different* means for the different groups. In the context of a regression model, this is a comparison between a model that only contains an intercept, and a model containing an intercept *and* dummy variables for the categories. Typically, these are called the *null model* and the *full model*, so we have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathcal{M}_{0} &: y_{i\\phantom{j}}  = \\mu_{\\phantom{j}}     + \\epsilon_{i}  &\\quad \\text{Null Model} \\\\\n",
    "    \\mathcal{M}_{1} &: y_{ij} = \\mu_{j} + \\epsilon_{ij} &\\quad \\text{Full Model} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In `R`, sticking with our `mtcars` example, we could specify these in the following way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bca320d8",
   "metadata": {
    "tags": [
     "remove-cell"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "data(mtcars)\n",
    "mtcars$origin <- c('Japan','Japan','USA','USA','USA','USA','USA','Europe','Europe',\n",
    "                   'Europe','Europe','Europe','Europe','Europe','USA','USA','USA',\n",
    "                   'Europe','Japan','Japan','Japan','USA','USA','USA','USA',\n",
    "                   'Europe','Europe','Europe','USA','Europe','Europe','Europe')\n",
    "mtcars$origin <- as.factor(mtcars$origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f101c14",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "null.mod <- lm(mpg ~ 1,      data=mtcars) # null model\n",
    "full.mod <- lm(mpg ~ origin, data=mtcars) # full model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3333cfcd",
   "metadata": {},
   "source": [
    "Thus, the full model contains our categorical variable of interest and the null model does not. If our question revolves around whether this predictor is actually necessary, a natural way to do so would be to compare how well each model fits the data. If the null and full model have a similar model fit, then it suggests that `origin` is doing little more than an intercept. In other words, the groups means appear largely identical. Alternatively, if the full model fits much better than the null model then it suggests that allowing the group means to differ is a more accurate reflection of the data. In other words, at least two of the group means appear to be different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b94e3e1",
   "metadata": {},
   "source": [
    "### Model Comparisons Using Sums-of-Squares\n",
    "In order to compare the model fits, the most intuitive method would be to calculate the the magnitude of the residual variance of each model and compare them. If a model fits better, its residual variance will be smaller. In other words, the average degree to which the data deviates from the model prediction will be reduced. Unfortunately, there is a numeric problem with this. What we really want to do is figure out how much the model has improved between the null and the full. In other words, what is the magnitude of variance associated with our effect of interest. Intutively, what we might think about doing is calculating\n",
    "\n",
    "$$\n",
    "\\Delta\\sigma^{2} = \\sigma^{2}_{\\text{null}} - \\sigma^{2}_{\\text{full}}.\n",
    "$$\n",
    "\n",
    "At the level of the *population*, this would work. However, at the level of *sample estimates*, this is a messy quantity that does not cleanly isolate the improvement added by the full model. To see why, consider the expanded definition of the variances as calculated from a sample\n",
    "\n",
    "$$\n",
    "\\Delta\\hat{\\sigma}^{2} = \\underbrace{\\frac{\\sum{\\epsilon_{0}^{2}}}{n - p_{0}}}_{\\hat{\\sigma}^{2}_{\\text{null}}} - \\underbrace{\\frac{\\sum{\\epsilon_{1}^{2}}}{n - p_{1}}}_{\\hat{\\sigma}^{2}_{\\text{full}}},\n",
    "$$\n",
    "\n",
    "where $\\epsilon_{0}$ and $\\epsilon_{1}$ are the residuals from the null and full models, and $p_{0}$ and $p_{1}$ are the number of parameters from the null and full models. Written this way, it is clear that we are subtracting quantities with different denominators. If we had the whole population at our disposal, each quantity would be divided by $n$ and there would be no problem. However, because these are *estimates*, we lose a degree of freedom for each parameter we have to estimate. Because the full and null models have different numbers of parameters, this makes the final value very unclear in terms of what it is actually telling us about the differences between these models. This is precisely because, in a sample, the individual terms are scaled differently. As such, even if the model fits were *identical*, we may still see a difference here due to differences in the number of parameters.\n",
    "\n",
    "The solution then is to *not* use the residual variances to compare the models, despite its intuitive appeal. Instead, we forego the denominators entirely to give\n",
    "\n",
    "$$\n",
    "SS_{\\text{effect}} = \\sum{\\epsilon_{0}^{2}} - \\sum{\\epsilon_{1}^{2}}.\n",
    "$$\n",
    "\n",
    "This value is more usually known as the *between-groups sum-of-squares* ($SS_{B}$) and is formed from the *residual sum-of-squares* (RSS) from each model. The logic is that we cannot isolate the magnitude of the model improvement by using the sample error variances because\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^{2}_{\\text{effect}} \\neq \\hat{\\sigma}^{2}_{\\text{null}} - \\hat{\\sigma}^{2}_{\\text{full}}.\n",
    "$$\n",
    "\n",
    "But we *can* using the sums-of-squares because\n",
    "\n",
    "$$\n",
    "SS_{\\text{effect}} = RSS_{\\text{null}} - RSS_{\\text{full}}.\n",
    "$$\n",
    "\n",
    "This is a direct consequence of removing the messiness of the denominators from the calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89db1ba2",
   "metadata": {},
   "source": [
    "Let us see this in practice for our two example models. In the code below, we can calculate the residual sums-of-squares as a measure of each model fit. The *larger* the value, the more error remains and the *worse* the model fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f4ea7d",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1126.0472  732.1721\n"
     ]
    }
   ],
   "source": [
    "null.RSS <- sum(resid(null.mod)^2)\n",
    "full.RSS <- sum(resid(full.mod)^2)\n",
    "\n",
    "print(c(null.RSS,full.RSS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b953fa40",
   "metadata": {},
   "source": [
    "So, we can see already that the null model has a larger RSS and thus is a worse fit to the data. The difference between these values therefore tells us how much the error *reduces* after including `origin` in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31dccaeb",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 393.8751\n"
     ]
    }
   ],
   "source": [
    "SS.B <- null.RSS - full.RSS\n",
    "\n",
    "print(SS.B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359e9305",
   "metadata": {},
   "source": [
    "How do we interpret this value? Generally, we cannot because the units are not any sort of standardised scale. However, what we can do is compare this value *relative* to some other measure on the same scale. What measure to use? Well, at present, the $SS_{B}$ gives us a sense of how much we have reduced the error of the null model by including a specific predictor. What we want to know is whether this is *large* or *small*? More generally, we want to know whether any improvement we have seen is likely to be random noise (in which case the predictor has done *nothing* to help the model) or whether it implies something more systematic about the data (i.e. that the population means differ across the groups).\n",
    "\n",
    "So, we need some measure of noise to compare $SS_{B}$ to. In general, we have two options: $RSS_{\\text{null}}$ or $RSS_{\\text{full}}$. These are both sums-of-sqaures that reflect the *error* or *noise*, as they capture the degree of variability that is left-over within each model. If it were the case that the predictor was *not* related to the data then these quantities would be similar. However, if the predictor *was* related to the data then the null model would be missing a key component and its error would be larger than it should be. This would make $RSS_{\\text{null}}$ a poor comparison. Because of this, $RSS_{\\text{full}}$ becomes the much better choice. If the predictor *does* relate to the data then $RSS_{\\text{full}}$ will be the correct magnitude, but if the predictor *does not* relate to the data then $RSS_{\\text{full}}$ will be similar to $RSS_{\\text{null}}$. So either way, $RSS_{\\text{full}}$ becomes the better measure of *error* or *noise*.\n",
    "\n",
    "Within the context of an ANOVA model, $RSS_{\\text{full}}$ is more usually known as the *within-group sum-of-squares*, or $SS_{W}$ for short, because it effectively measures variability in terms of individual subjects around the group means. In other words, how well do the group means fit the data? Within our example in `R`, we already calculated $RSS_{\\text{full}}$ and so can simply copy it to a new variable with a different name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd6444a1",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 732.1721\n"
     ]
    }
   ],
   "source": [
    "SS.W <- full.RSS\n",
    "print(SS.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c82eb91",
   "metadata": {},
   "source": [
    "### The $F$-ratio\n",
    "Given what we have discussed above, we could just eye-ball the magnitude of $SS_{B}$ relative to $SS_{W}$. However, a more principled approach is to combine these values into some sort of *test statistic*, akin to the $t$-statistic we have discussed previously. In doing so, we can combine the two sources of information into a single standardised value that can be interpreted across different datasets. Much in the same way that a $t$-statistic is formed by dividing an *effect* by *error*, we could similarly form a statistic by calculating $\\frac{SS_{B}}{SS_{W}}$. This would tell us how large the improvement in model fit is, relative to the amount of error left over. \n",
    "\n",
    "The statistic we use for this purpose is known as $F$ (named after *Fisher*). Given the description above, it may seem like we could calculate $F = \\frac{SS_{B}}{SS_{W}}$. However, there is some additional complexity. Although both $SS_{B}$ and $SS_{W}$ appear to be compatible, because they are both sums-of-squares, they cannot be directly compared. This is because they are both *sums*, and so their magnitude is dependant upon whatever we are summing over. If they were both summed over the same element of the model, there would be no problem. But this is not the case. As we will see below, the magnitude of $SS_{B}$ depends upon the number of model parameters, yet the magnitude of $SS_{W}$ depends upon the sample size. So both terms need to be *scaled* first, to make them truly comparable. These scaled values are known as *mean squares*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716927cd",
   "metadata": {},
   "source": [
    "#### Mean-squares\n",
    "Starting with $SS_{B}$, this value can be interpreted as the improvement in model fit after adding a certain number of parameters to the model. Unfortunately, this value will always scale with the number of parameters we add. This is because the more parameters we add, the better the fit can become. The most extreme example of this is having one parameter for every observation. In this case, we could make the model fit *perfectly* with no error. Thus, we can always make the model better and better by adding more and more parameters[^overfit-foot]. The problem is that we do not know whether $SS_{B}$ is large because of a genuine improvement to the model fit, or simply because we have added more parameters. \n",
    "\n",
    "To make this value more meaningful, we can therefore divide it by the number of parameters we have added to give an *average improvement per-parameter*. This has the advantage of turning the sums-of-squares back into a measure of variance, but one that is now correctly calculated to give $\\hat{\\sigma}^{2}_{\\text{effect}}$. The number of additional parameters is known as the *numerator degrees of freedom* (or $df_{1}$) and the scaled version of $SS_{B}$ is known as the *between-group mean square*, or $MS_{B}$ for short. As such \n",
    "\n",
    "$$\n",
    "MS_{B} = \\frac{SS_{B}}{df_{1}} = \\frac{SS_{B}}{k - 1} = \\hat{\\sigma}^{2}_{\\text{effect}}.\n",
    "$$\n",
    "\n",
    "In the case of the `mtcars` example, we need to add $k - 1 = 2$ parameters to the model to capture the different group means. As such, we divide $SS_{B}$ by $df_{1} = 2$ to produce the $MS_{B}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41e48c77",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 196.9376\n"
     ]
    }
   ],
   "source": [
    "df.1 <- 2\n",
    "MS.B <- SS.B / df.1\n",
    "print(MS.B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5280c1",
   "metadata": {},
   "source": [
    "So, we have an improvement in model fit in terms of reducing the error variance by an average of 196.94 for each parameter we have added."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad470b9",
   "metadata": {},
   "source": [
    "\n",
    "Moving on to $SS_{W}$, we have a similar problem because this value always scales with *sample size*. Remember, this is simply the residual sum-of-sqaures for the full model. As such, the more residuals we have, the larger the sum will be. As noted above for $SS_{B}$, we therefore do not know whether $SS_{W}$ is large because the model fit is poor, or because we have a large $n$.\n",
    "\n",
    "Using a similar logic to above, it would seem that we could just divide $SS_{W}$ by $n$. However, there is a catch. Remember that dividing a sums-of-squares by some value gives us a *mean-square*, but that mean-squares are effectively *variances*. So what variance do we get from the residual sum-of-squares? As covered in previous weeks, this is simply the estimated *error variance* of the model, $\\hat{\\sigma}^{2}$. However, we also know from previous weeks that dividing by $n$ leads to a *biased* estimate of the population variance and that, as a generalisation of Bessel's correction, we must divide by $n-p$. As such, rather than dividing $SS_{W}$ by $n$, we divide by $n-p$, which gives the *denominator* degrees of freedom $df_{2} = n - p$. This gives the *within-groups mean-square*, or $MS_{W}$ for short \n",
    "\n",
    "$$\n",
    "MS_{W} = \\frac{SS_{W}}{df_{2}} = \\frac{SS_{W}}{n-p} = \\hat{\\sigma}^{2}_{\\text{error}}.\n",
    "$$\n",
    "\n",
    "We can see this in our `mtcars` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30e87f9",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 25.24731\n"
     ]
    }
   ],
   "source": [
    "df.2 <- full.mod$df.residual # this is always n - p\n",
    "MS.W <- SS.W / df.2\n",
    "print(MS.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d6830f",
   "metadata": {},
   "source": [
    "Which, as already indicated, is *identical* to the error variance from the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "168f7e2b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 25.24731\n"
     ]
    }
   ],
   "source": [
    "sigma <- summary(full.mod)$sigma\n",
    "print(sigma^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df514a5e",
   "metadata": {},
   "source": [
    "This gives us the left-over variance after fitting the full model. So together, we have a reduction in residual variance of 196.94 after adding `origin` to the model, with only a residual variance of 25.25 left over. If our reduction in variance were of a similar magnitude to the amount left over, we could not say with any certainty that it was a true effect because the \"effect\" and the \"error\" look basically the same. However, in this case, the reduction in error variance is *much larger* than the amount left over, which is already suggestive of some true effect and thus that including `origin` in the model allows for a much better model fit. Because the `origin` factor allows for the group means to differ, this implies that *at least* one pair of group means are different from each other. This connects this result directly to our original omnibus hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929f990f",
   "metadata": {},
   "source": [
    "```{admonition} What's in a name?\n",
    ":class: tip\n",
    "As we can see, some of the confusion around ANOVAs and ANOVA calculations simply comes down to using *different names* for familiar quantities. The name ANOVA tells us that the method involves comparing variances, yet those variances are called *mean-squares*. Usually, we would not give a name to the intermediate quantities produced during the calculation of variances, but the ANOVA names them as *sums-of-squares*. These quantities take on more importance within an ANOVA, because the sums-of-squares are the only meaningful way to compare two model fits, before they can be scaled back into variances. However, this creates a confusing collection of sums-of-squares, mean squares and degrees of freedom. \n",
    "\n",
    "*Remember*, all we are doing is comparing two models using their residuals. That is it. Try not to get too lost in the specifics of *how* this is done and keep focus on the very simple aim of the ANOVA. Remember as well that the ANOVA table was introduced by Fisher as a convenient way of organising all the arithmetic. The table was designed to make the calculations less confusing, but the table is *not* the ANOVA itself. There is nothing in an ANOVA table that implies any form of model comparison, but this is exactly what it is showing. As we will see below, in `R` such a table can be produce by *directly* comparing two models, making this perspective event clearer.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a28597",
   "metadata": {},
   "source": [
    "#### The Definition of $F$\n",
    "So, given everything we have discussed above, we have taken a rather long and winding road to the definition of the $F$-statistic, which is:\n",
    "\n",
    "$$\n",
    "F = \\frac{SS_{B} / df_{1}}{SS_{W} / df_{2}} = \\frac{MS_{B}}{MS_{W}} = \\frac{\\hat{\\sigma}^{2}_{\\text{effect}}}{\\hat{\\sigma}^{2}_{\\text{error}}}.\n",
    "$$\n",
    "\n",
    "At its most basic, the $F$-ratio is compaing two measures of variance. Hence the name, Analysis of *Variance*. If we want to make this even simpler, we can say \n",
    "\n",
    "$$\n",
    "F = \\frac{\\text{Improvement in error between the null and full models}}{\\text{Error remaining}}.\n",
    "$$\n",
    "\n",
    "Or, to put this a difference way, how much variance have we managed to explain for each parameter we have added compared to how much we still have left to explain?\n",
    "\n",
    "Given that $F$ is a *ratio*, we can think of its value in fairly simple terms. If $F = 1$ then the magnitude of the improvement is the same as the magnitude of the error. This is what we would expect if the null hypothesis were true, because any improvement is indistinguishable from normal sampling error. However, if $F > 1$, it means that the improvement is much larger than the error. The bigger this value becomes, the more the improvement dwarfs the error and the harder it becomes to believe that all we have captured is noise.\n",
    "\n",
    "Because the $F$ is the ratio of two *estimates* ($\\hat{\\sigma}^{2}_{\\text{effect}}$ and $\\hat{\\sigma}^{2}_{\\text{error}}$), and because each estimate is a *random variable*, the $F$-ratio itself is a random variable with some distribution. Under the null hypothesis that there is no effect, $\\sigma^{2}_{\\text{effect}} = \\sigma^{2}_{\\text{error}}$ and the $F$-ratio will be 1 on average, as indicated above. The null $F$-distribution therefore has a form where the expected value is 1. This makes the null $F$-distribution only *one-tailed*, as illustrated below for an example $F_{2,29}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "082b24da",
   "metadata": {
    "tags": [
     "remove-input"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Plot with title “~”\n",
       "Plot with title “Null F-distribution, ”\n",
       "Plot with title “F[2 * \",\" * 29]”"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameters\n",
    "df1 <- 2   # numerator degrees of freedom\n",
    "df2 <- 29  # denominator degrees of freedom\n",
    "\n",
    "# Range of F values to plot\n",
    "x <- seq(0, 6, length.out = 200)  # up to ~6 is enough for df1=2, df2=30\n",
    "\n",
    "# Density of the F-distribution\n",
    "y <- df(x, df1, df2)\n",
    "\n",
    "# Plot\n",
    "plot(x, y, type = \"l\",\n",
    "     lwd = 2, col = \"blue\",\n",
    "     main = bquote(\"Null F-distribution, \" ~ F[2*\",\"*29]),\n",
    "     xlab = \"F-value\", ylab = \"Density\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd55d9c",
   "metadata": {},
   "source": [
    "If we are following NHST convention, we can therefore compare the value of $F$ we have calculated to the null distribution and produce an associated $p$-value. As with the $t$-statistic, the larger $F$ becomes, the less probable it is if the null were true and thus the *smaller* the $p$-value becomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63b2fc4",
   "metadata": {},
   "source": [
    "#### Summary of Calculations\n",
    "\n",
    "Because the steps needed to calculate $F$ are quite involved, we have summarised all of them in the `R` code below. This will hopefully make it clear that we are really comparing two models to each other, it is just that we have to go about it in a slightly complicated way in order for the numbers to make sense. Also, do not worry, we will see how to automate all of this in `R` very shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5bcac81",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  F.ratio     p.value\n",
      " 7.800338 0.001946794\n"
     ]
    }
   ],
   "source": [
    "# Null model and full model\n",
    "null.mod <- lm(mpg ~ 1,      data=mtcars)\n",
    "full.mod <- lm(mpg ~ origin, data=mtcars)\n",
    "\n",
    "# Residual sums-of-squares\n",
    "null.RSS  <- sum(resid(null.mod)^2)\n",
    "full.RSS  <- sum(resid(full.mod)^2)\n",
    "\n",
    "# Sums-of-squares\n",
    "SS.B <- null.RSS - full.RSS # between-groups (model improvement)\n",
    "SS.W <- full.RSS            # within-groups (error)\n",
    "\n",
    "# Mean-squares\n",
    "df.1 <- 2\n",
    "df.2 <- full.mod$df.residual # (n-p)\n",
    "MS.B <- SS.B / df.1          # reduction in error variance (effect)\n",
    "MS.W <- SS.W / df.2          # remnaining error variance (error)\n",
    "\n",
    "# F-ratio\n",
    "F <- MS.B / MS.W\n",
    "\n",
    "# p-value from null F-distribution with df1 and df2\n",
    "p <- pf(q=F, df1=df.1, df2=df.2, lower.tail=FALSE)\n",
    "\n",
    "# Results\n",
    "print(data.frame(\"F.ratio\"=F, \"p.value\"=p), row.names=FALSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7bb4d8",
   "metadata": {},
   "source": [
    "The value of $F$ indicates that the reduction in error between the null and full model is nearly 8 times *larger* than we would expect, if this effect was just noise. In other words, this is nearly 8 times bigger than our expectation under the null hypothesis of no differences between the group means. The probability of achieving a value of $F_{2,29} = 7.80$ if the null were true is $p = 0.002$, which is below the usual NHST threshold of $p = 0.05$, thus this would be declared \"significant\" within this framework. In terms of our omnibus hypothesis, this is taken to imply that *at least one* of the mean differences between the levels of `origin` is also significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2271ce6a",
   "metadata": {},
   "source": [
    "## ANOVA Tables in `R`\n",
    "\n",
    "### The `anova()` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b9625a3",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of Variance Table\n",
      "\n",
      "Model 1: mpg ~ 1\n",
      "Model 2: mpg ~ origin\n",
      "  Res.Df     RSS Df Sum of Sq      F   Pr(>F)   \n",
      "1     31 1126.05                                \n",
      "2     29  732.17  2    393.88 7.8003 0.001947 **\n",
      "---\n",
      "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n"
     ]
    }
   ],
   "source": [
    "print(anova(null.mod, full.mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdd36bc",
   "metadata": {},
   "source": [
    "Within this table, we should see all the values that we calculated manually above, alongside the $p$-value for the $F$-ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee87bae4",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single term deletions\n",
      "\n",
      "Model:\n",
      "mpg ~ origin\n",
      "       Df Sum of Sq     RSS    AIC F value   Pr(>F)   \n",
      "<none>               732.17 106.17                    \n",
      "origin  2    393.88 1126.05 115.94  7.8003 0.001947 **\n",
      "---\n",
      "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n"
     ]
    }
   ],
   "source": [
    "print(drop1(full.mod, test=\"F\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888e4fb3",
   "metadata": {},
   "source": [
    "### The `Anova()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36e6a714",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: carData\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anova Table (Type II tests)\n",
      "\n",
      "Response: mpg\n",
      "          Sum Sq Df F value   Pr(>F)   \n",
      "origin    393.88  2  7.8003 0.001947 **\n",
      "Residuals 732.17 29                    \n",
      "---\n",
      "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n"
     ]
    }
   ],
   "source": [
    "library(car)\n",
    "print(Anova(full.mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08145828",
   "metadata": {},
   "source": [
    "... Notice that this table says `Type II tests` at the top. This is something we will explore in more detail in the associated synchronous session next week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ead05f",
   "metadata": {},
   "source": [
    "... So, the core understanding here is that the ANOVA is simply a *model comparison procedure*. The ANOVA effects usually reported within an ANOVA table are really comparisons between a model that contains that effect and a model that does not contain that effect. The *variance* within the name ANOVA is a comparison between the error variances of these models. In other words, the ANOVA is asking the question \"how much does the error reduce when we include this effect in the model?\" The table is just a helpful way of organising all these comparisons. So we can see that the ANOVA model is in fact just a form of multiple regression, and the tests associated with the ANOVA are summaries of model comparisons. This is the true way to conceptualise an ANOVA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb578a33",
   "metadata": {},
   "source": [
    "## The Regression ANOVA $F$-test\n",
    "... In fact, this test is already provided at the bottom of the regression table...\n",
    "\n",
    "Note that this will not always align with the ANOVA tests we want. For simple models that only contain a single factor and *nothing else*, the regression ANOVA test will be equivalent to an omnibus test on that single factor. For models that contain *multiple* factors and/or continuous predictors, this will not be equivalent. As such, it can be good practise to produce the ANOVA table anyway, even if in some cases it is redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1007d828",
   "metadata": {},
   "source": [
    "[^overfit-foot]: You may wonder what the problem is with this. Surely we want the model to fit as well as possible? However, a *perfect* fit suggests *over-fitting*. Remember, our aim is to use our model to separate those effects that are universal to our population of interest from the noise. A model that fits one specific dataset does not do this. In fact, it will be fitting *both* the effects we are interested in *and* the noise. This may result in a perfect fit for that specific dataset, but it tells us nothing about our population of interest and certainly would not fit another sample very well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a41133d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}