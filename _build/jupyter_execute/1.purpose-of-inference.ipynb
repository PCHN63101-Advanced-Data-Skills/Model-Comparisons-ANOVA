{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6b3fd6-1753-45e7-a920-fccf68cbdcac",
   "metadata": {},
   "source": [
    "# The Purpose of Statistical Inference\n",
    "In our previous weeks, we have established a very general framework for statistical modelling and have examined one particular case in the form of simple/multiple regression. Throughout all of this, our aim was to say something about a *population* on the basis of a *sample*. At the end of the previous lesson, we were left with an estimated model that we visualised and interpreted in a very general sense. However, it is important to remind ourseleves that we are not really interested in the *estimates* per-se. What we are interested in is the *population values*. We may well have a slope estimate of $\\hat{\\beta}_{1} = -3.80$, but our real question is whether the *real* slope is equal to $-3.80$, or whether $-3.80$ is even close to the real value? In general saying something about only a single specific group of measurements is not very useful scientifically. However, saying something much more general *is* very useful because we can make predictions about the future, allowing us to implement changes that have real-world consequences and develop theories that apply to the population as a whole. Our desire is therefore to go from results in one specific sample and generalise much more widely. This seems a noble aim. However, there are some deep philosophical issues with attempting to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61418881",
   "metadata": {},
   "source": [
    "## Deductive vs Inductive Reasoning\n",
    "Deductive and inductive reasoning are two different approaches to reaching conclusions on the basis of logic. Deductive reasoning is based on starting with a general premise and then reaching a specific conclusion as a logical consequence. For instance:\n",
    "\n",
    "1. All humans are mortal.\n",
    "2. George is a human.\n",
    "3. Therefore, George is mortal.\n",
    "\n",
    "Here we start with a very general premise (*all humans are mortal*) and arrive at a specific example (*George is mortal*). If the premise is true, then the conclusion *has* to be true. We do not need to test this, we can arrive at this conclusion using pure reason[^george-foot]. Because of this, as long as our premise is correct, we can happily go from the *general* to the *specific* without a logical misstep. \n",
    "\n",
    "This type of reasoning can also take different forms. For instance, the *modus tollens* is a type of deductive argument where a condition is false because its expected outcome is false. For instance:\n",
    "\n",
    "1. If it is raining, the ground will be wet.\n",
    "2. The ground is not wet.\n",
    "3. Therefore, it is not raining.\n",
    "\n",
    "There is no way around this. If a logic consequence of rain is that the ground is wet, then the ground being dry *must* indicate that it is *not* raining. The only way this could be wrong is if our premise is wrong. This is what makes mathematics so powerful. Because *every* result is a deductive consequence of its axioms[^axiom-foot], mathematics is the most powerful deductive force in the world. So long as the axioms are true, everything else *has* to be true.\n",
    "\n",
    "Inductive reasoning, on the other hand, goes from the *specific* to the *general*. It involves generalising from what we have observed so far, and assuming that all cases we will observe in the future will be the same. For instance:\n",
    "\n",
    "1. All life forms so far discovered are composed of cells.\n",
    "2. Therefore, all life forms are composed of cells.\n",
    "\n",
    "This is exactly the form of reasoning that typifies scientific investigation. We conduct experiments and then generalise to all future observations. If we do an experiment enough times and get the same result, we assume all future attempts will lead to the same result. Unfortunately, unlike deductive reasoning, this is *not logically valid*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47953256",
   "metadata": {},
   "source": [
    "## The Problem of Induction\n",
    "\n",
    "```{figure} images/hulme.png\n",
    "---\n",
    "scale: 25%\n",
    "align: left\n",
    "---\n",
    "```\n",
    "\n",
    "The *problem of induction* refers to the logical issues surrounding the use of inductive reasoning in science, and was first articuled in 1739 by the Scottish philosopher [David Hume](https://en.wikipedia.org/wiki/David_Hume). The problem refers to the fact that there is no non-circular way of justifying why the specific cases of a phenomena observed thus far should give rise to the same phenomena in the future. The best we can often do is to appeal to the *uniformity of nature* and suggest that things must continue the way they have, because things always have. However, we know first hand that this is not a great argument, because things do not always have to proceed the way they have in the past. A pithy quote from [Bertrand Russell](https://en.wikipedia.org/wiki/Bertrand_Russell) makes this point clear:\n",
    "\n",
    "```{epigraph}\n",
    "Domestic animals expect food when they see the person who usually feeds them. We know that all these rather crude expectations of uniformity are liable to be misleading. The man who has fed the chicken every day throughout its life at last wrings its neck instead, showing that more refined views as to the uniformity of nature would have been useful to the chicken.\n",
    "\n",
    "-- Bertrand Russell, *The Problems of Philosophy* (1912)\n",
    "```\n",
    "\n",
    "As an example, why do you believe the sun will rise tomorrow morning? The answer is probably because it has always risen in the past. However, this is a circular argument because it relies on induction to justify induction. To make this clearer, the argument is\n",
    "\n",
    "1. The sun has risen every day in the past\n",
    "2. Therefore, the sun will rise tomorrow\n",
    "\n",
    "```{figure} images/circular-reasoning.jpg\n",
    "---\n",
    "scale: 40%\n",
    "align: right\n",
    "---\n",
    "```\n",
    "\n",
    "But to get from 1 to 2, you have to make the assumption that the future will always represent the past. You have justified the assumption that the future resembles the past by appealing to the fact that it has resembled the past in the past. The conclusion therefore cannot be reached with pure logic because it relies on an extra assumption. Or, to put it another way:\n",
    "\n",
    "\"predicting the future from the past works because predicting the future from the past works...\" \n",
    "\n",
    "We have justified our logic using the logic we are trying to justify. As such, Hume's insight is that you can *never* deduce future regularities from past experience because this relies on circular reasoning[^probfoot]. So in what way are we ever able to logically justify reaching general conclusions from specific instances? This is not just an issue for statistics. Hume's insight is so devastating because it puts the whole enterprise of science into question. Far from being the most rational system we have for reaching conclusions about the world, it seems that science is, in fact, fundamentally *irrational*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6193016",
   "metadata": {},
   "source": [
    "## Statistical Inferences as Induction\n",
    "To focus-in on our main theme, the issue we have is that statistical inference is fundamentally and inescapably an exercise in inductive reasoning. We are taking isolated examples of our population of interest and then extrapolating to the population at large. We are going from the *specifics* of our sample to the *general* case of the population. This cannot be justified on the basis of pure logic. In making predictions about the future, we are assuming that nature is uniform and that we can repeatedly sample from the population and the same patterns will hold. Even more fundamentally, the whole notion of *long-run frequency*, which is at the heart of Frequentist interpetations of probability, relies wholly on inductive logic to allow past frequncies of events to reflect the future. Although we started this section with the noble desire of using our sample to try and say something about the population, it should now be clear that this cannot be logically or rationally justfied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2f689c",
   "metadata": {},
   "source": [
    "\n",
    "```{admonition} Frequentist vs Bayesian Statistics\n",
    ":class: tip\n",
    "Within statistical science, there is a core friction between two different interpretations of probability. This friction has led to a divide in terms of the development of two different approaches to statistics, known as *Frequentism* and *Bayesianism*. Frequentist methods were developed within the early 20th-Century by Fisher, and consist of the vast majority of statistical concepts that are taught and used in modern science. Bayesian methods are much older and have been side-lined for many years, with a resurgance in their use only happening more recently. \n",
    "\n",
    "For a Frequentist, probability is a physical property of the universe that is directly observable. For instance, if we roll a fair die then the probability of any one number is $\\frac{1}{6}$. This number is precisely the long-run frequency of the outcomes. If you rolled the die many many times over, then the frequency with which each number appears would get closer and closer to $\\frac{1}{6}$. For a Frequentist, all probability statements reflect some long-run frequency of an outcome. Even when we talk about probability in relation to an experiment, we concieve of running the same experiment a large number of times and counting the frequency of different outcomes. This interpretation of probability appears sensible and grounded in reality. However, as we will come to see, this can be overly restrictive in practise.\n",
    "\n",
    "For a Bayesian, probability reflects our *degree of belief*. In other words, probability is a quantification of our uncertainty about the world. This is not some physical property that can be counted. Instead, this is a weighting of evidence. Given our current knowledge and our current data, what do we believe about the future? A Bayesian can ask the question \"what is the probability of rain tomorrow?\" For a Frequentist, this is meaningless because “tomorrow” is a one-time event. You cannot repeat \"tomorrow\" under identical conditions. You cannot generate a long-run frequency of how often a specific date within a specific year in a specific place has had rain, as it only happens once. For a Bayesian, the question does make sense because we are asking \"given our current knowledge and the data we have available, how much do we *believe* that it will rain tomorrow?\" This quantifies our degree of uncertainty, using all the available information[^bayes-foot].\n",
    "\n",
    "To core tension between these methods comes form the fact that Bayesian probability relies on a somewhat ill-defined and non-physical property of our beliefs. This makes Bayesianism seem distasteful to some. This is especially true when you take the view that subjectivity has no place in science. Bayesian probabilities *require* a specification of your own personal beliefs about the probability of a phenomena, prior to seeing any data. For instance, when calculating the probability of rain in Manchester tomorrow, we need to specify our prior beliefs about rain in Manchester. We may be tempted to say that this possibility is very high, because Manchester is a rainy place. However, this is driven by stereotype and personal belief, *not* by data. Bayesian proability weights this belief against the data (e.g. readings from local weather stations) and comes to a conclusion about our uncertainty. But this uncertainty is driven, in part, by our subjective beliefs. It is this accuasation of subjectivity that made Fisher dislike Bayesianism and led him to develop Frequentist methods as an alternative. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a272ca23",
   "metadata": {},
   "source": [
    "In terms of statistics, the important point here is that *inference is where the controversy lies*. There is nothing controversial about building a statistical model and then finding parameter estimates that make the data as probable as possible. What *is* controversial is trying to say something about the population as a whole based on that model. Indeed, arguments within statistics about Frequentist vs Bayesian approaches are often centred on how these different paradigms treat inductive inference. So, we have been fully justified in everything we have done so far, as our results are based on logical deductive consequences of our modelling assumptions. However, in our desire to go beyond the model we have developed, we hit a philosophical problem. How can we ever rationally justify making generalisations about an unobserved population based on a small part of it? This is the tension that lies at the heart of statistical inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372c3390",
   "metadata": {},
   "source": [
    "## Are There Any Solutions to Induction?\n",
    "As we have now seen, it has been over 250 years since Hume articulated the problem of induction, and yet science continues. So how has this problem been solved? In short, it has not. In fact, there is *no* solution to the problem of induction that has been successfully articulated. If we ironically apply the logic of induction, this suggests that no solution will ever be found. So how can science continue if its most basic principles are logically invalid? In this final section, we will have a look at some choice \"solutions\" to induction. Although we have already prefaced this section by saying that *no* solution to induction exists, that has not stopped philosophers of science from trying. The two major suggestions that are of direct relevance here are Karl Popper's principle of *falsification* and the Bayesian perspective on induction. This is mainly because both of these are *practical* considerations of the problem, rather than metaphysical ones (such as [Kant's Transcendentalism](https://plato.stanford.edu/entries/kant-hume-causality/)). Neither of these approaches successfully solves induction, as one attempts to side-step induction whilst the other embraces a formal system for induction. However, both are highly influential ideas in philosophy of science.\n",
    "\n",
    "```{epigraph}\n",
    "It seems, then — and this is no longer controversial — that there is no solution to the problem of induction that could demonstrate with logical certainty the truth of general scientific theories.\n",
    "\n",
    "-- Howsen & Urbach, *Scientific Reasoning: The Bayesian Approach* (2006)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46ad047",
   "metadata": {},
   "source": [
    "### Popper's Falsificationism\n",
    "\n",
    "```{figure} images/popper.jpg\n",
    "---\n",
    "scale: 40%\n",
    "align: left\n",
    "---\n",
    "```\n",
    "\n",
    "In his 1959 book [The Logic of Scientific Discovery](https://www.librarysearch.manchester.ac.uk/permalink/44MAN_INST/1r887gn/alma9953976500001631), the German philosopher Karl Popper set out his solution to Hulme's problem of induction. Although Popper agreed that no amount of confirmatory evidence could ever *prove* a theory correct, he noted that a single piece of *disconfirmatory* evidence could render a theory *false*. As such, Popper reimagined the purpose of science not as an exercise in induction, but rather an exercise in *falsification*. If, over time, a theory resisted more and more precise attempts at falsification then Popper considered the theory to be *corroborated*, though never outright proven. As such, the theories that survive over time are the ones that have so far resisted falsification. Our aim, therefore, is to create theories that are falsifiable, and then attempt to falsify them. If they are falsified, we throw them away, if they resist falsification then we can say that they have thusfar been corroborated. \n",
    "\n",
    "Although Popper's idea of falsification can seem appealing at first, it is not without problems. Perhaps the biggest issue is that Popper's idea of how science *ought* to behave was not always compatible with how science *does* behave. For instance, it is rare that disconformatory evidence ever leads to a theory being outright abandoned. This is because there could be many reasons why a result has failed. Yes, the theory could be wrong, or it could be an issue with the data, or the precision of the measurement instrument, or human error, or a misunderstanding of certain assumptions. So without solid falsification, we just end up with a corpus of ambiguous findings. Scientists also do not just abandon theories at the first sign of trouble. The early experiments into Einstein's theory of relativity appeared inconsistent, but the theory was not suddenly abandoned. This is because the degree of deviation from the theory is not easy to quantify. Popper's view was that a single piece of evidence could shatter a theory, but in reality there is much ambiguity about what would actually count as disconformatory evidence. Furthermore, many competing theories can survive falsification, so how do we choose between them? In amassing corroborative evidence for one theory, we also corroborate other theories as well. So how do we use the logic of falsificationism to determine which theory to give preference to?\n",
    "\n",
    "Even if we put these issues aside, perhaps the biggest problem with falsificationism is that it sneaks induction in through the back door. If we are using theories that have not yet been falsified, then we are implicitly endorsing the idea that the theory will continue to not be falsified. Otherwise, why would we be using it? If we are happy to use a current theory to make general predictions about the world because it has been thusfar corroborated, then we are engagaing in inductive logic. Our assumption is that a theory gets stronger the more attempts at falsification it survives. But this is assuming that past events will continue into the future. In other words, the continued use of unfalsified theories about the world is arguably an inductive act, even if Popper would refuse to admit it. So whilst Popper sought to reframe science in terms of falsification, we still require induction to justify why we keep the theories that have survived. We keep them because we believe they make accurate predictions about the future, which is based on assuming the continuity of nature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c9be04",
   "metadata": {},
   "source": [
    "### Bayesian Scientific Reasoning\n",
    "An alternative to Popperian falsificationism is not to try and side-step induction. Instead, we can embrace induction as a necessary fact of science and try to develop a framework around it. This is the domain of Bayesian Scientific Reasoning, most clear elluciated by the philosophers Colin Howsen and Peter Urbach in their 2006 book [Scientific Reasoning: The Bayesian Approach](https://www.librarysearch.manchester.ac.uk/permalink/44MAN_INST/bofker/alma992975897924201631). As explained above, Bayesians view probability as a metric of uncertainty that balances prior beliefs with new evidence. This framework can be applied more widely to reasoning about scientific theories. Our degree of belief in a theory is a balance between our existing knowledge of the theory and new data we collect. Thus the degree to which we believe a theory can predict the future is founded on the evidence we already have, as updated by new evidence. This is then a framework that *embraces* induction and the uncertainty around it. Furthermore, this matches intuitively with how we, as human beings, rationalise about the world. We have existing beliefs, we collect data and we update those beliefs. Bayesian Scientific Reasoning therefore reframes induction into an exercise in developing probabilistic beliefs about the world. We have a certain degree of certainty about the future, but also accept that we may be wrong. We know this cannot be deduced logically, but by combining our existing knowledge with data, we can make a prediction that we have a degree of personal certainty in.\n",
    "\n",
    "Importantly, this is *not* a solution to induction. This is induction *reframed*. We have to accept induction and make peace with the fact that it is not logically justifiable. This may not feel particularly satisfying, but given that induction appears insolvable, there is arguably little else we can do. Beyond this uncomfortable conclusion, there are also the usual concerns about how subjectivity enters Bayesian calculations of probability. The degree to which you find this idea distasteful depends very much on your own personal view on subjectivity in science, though it is arguable that this is a natural part of the scientific endevour. As we will see by the end of this lesson, an obsessive drive towards objectivity can actually be counter-productive. Nevertheless, subjectivity of any form can appear unscientific[^subjective-foot], but there is no getting around the need for priors when calculating Bayesian probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70db8c08",
   "metadata": {},
   "source": [
    "### Modern Science and Pragmatism\n",
    "So, out of Falsificationism and Bayesianism, which approach does modern science take? Well, arguably, modern science uses *neither* of these. Instead, modern science generally falls-back on the philosophical principle of *Pragmatism* to justify induction. The reason for bringing-up Falsificationism is because it is often touted as the best solution to induction yet, as we can see above, this is not the case. Furthermore, this approach is often conflated with *null hypothesis significance testing*, which is a topic we will turn to later in this lesson. The reason for bringing up Bayesianism is because this is the strongest framework we have for *accepting* induction, yet would require a major shift in scientific thinking towards a Bayesian paradigm. Though there is some movement in this direction, we are not yet there. As such, within our current framework, *Pragmatism* is the best we can do. \n",
    "\n",
    "The philosophical principle of Pragmatism is founded on the idea of *using what works in a practice*. The pragmatic answer to the problem of induction is that induction appears to work. In this way, its justification comes from the fact that inductive reasoning has already uncovered many apparent truths of the world. So, in a way, it does not matter if it is logically invalid in a formal sense. The actual practical application of induction appears justified by its own success. We can either using induction to uncover the truths of the universe, or decide that induction is not justifiable and throw all these insights away. Which would seem the more sensible? Pragmatism is therefore very much an *attitude* to the problem, rather than a formal solution. \n",
    "\n",
    "```{figure} images/reichenbach.jpg\n",
    "---\n",
    "scale: 55%\n",
    "align: right\n",
    "---\n",
    "```\n",
    "\n",
    "This argument was put forward by the German philosopher [Hans Reichenbach](https://en.wikipedia.org/wiki/Hans_Reichenbach), who concluded that of all the possible methods of inference, inductive inference should be the most successful. If nature is uniform, induction will work. If nature is not uniform then induction will not work, but nor will anything else. As such, we may as well *try* and use induction, knowing that it *could* be the most successful method of inference. Moreso, Reichenbach argues that induction is our only real choice. This argument is not based on trying to prove the validity of the method from formal logic, rather it is a *practical* argument about induction being our only choice that appears justified by its own success. In his own words:\n",
    "\n",
    "```{epigraph}\n",
    "If there is an order of nature which makes future events resemble past ones, the inductive method will find it. If there is no such order, the method will not work — but then, no method will work.\n",
    "\n",
    "-- Hans Reichenbach, *The Rise of Scientific Philosophy* (1951)\n",
    "```\n",
    "\n",
    "If we adopt Pragmatism then the problem of induction has not been solved, it has been *outmaneuvered*. This may feel unsatisfying, but it is the reality of where we are with this problem. Importantly, statistical inference as an exercise in induction *cannot* be rationalised on pure logical ground. We cannot \"prove\" anything using statistical inference. Indeed, from a hard-nosed logical perspective, every conclusion we reach is an irrational leap of faith that requires assumptions about the uniformity of nature that *cannot* be justified. Because of this, statistical inference is controversial and no singular method for inference can be considered \"correct\". We have one approach, called *null hypothesis significance testing*, that has gained much traction within the field of Experimental Psychology. However, this method has its own internal controversies and cannot be considered superior to any other inferential method in any formal sense. The best we can do is justify our approaches pragmatically. In order to be a strong data analyst and Experimental Psychologist, this lack of logical grounding for inductive reasoning is important to understand. We also need to understand how this state of affairs influences the methods we use to *try* and make generalisations from our data, even if technically we have no logical justification for doing so.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ca6137",
   "metadata": {},
   "source": [
    "`````{topic} What do you now know?\n",
    "In this section, we have explored the logical and philosophical topics that underlie what we are trying to achieve with statistical inference. After reading this section, you should have a good sense of:\n",
    "\n",
    "- The difference between *deductive* and *inductive* reasoning.\n",
    "- The problem of induction and how there is no non-circular way of justifying inductive logic.\n",
    "- How the problem of induction questions the logic of both statistical inference, as well as science in general.\n",
    "- The proposed solutions to induction, namely Falsificationism and Bayesianism, and why neither method can be considered a \"solution\".\n",
    "- The pragmatic argument in favour of induction that is most widely adopoted to justfy its continued use.\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06177402",
   "metadata": {},
   "source": [
    "[^probfoot]: You might think to skirt this issue by making it probabilistic. For instance, \"the sun has always risen in the past, therefore it is highly probable that it will rise tomorrow\". However, this has the same issue of circular reasoning because it requires the assumption that past frequencies of an event can justify a belief about the future. In doing so, our only argument is that this has worked in the past and we hit the same problem.\n",
    "\n",
    "[^george-foot]: Which is lucky for George.\n",
    "\n",
    "[^axiom-foot]: The axioms are the *rules* that govern mathematics. These are like the *premises* of all mathematics. So long as these are correct, everything that follows *must* be correct on purely logical grounds.\n",
    "\n",
    "[^bayes-foot]: As a point of clarity, a Bayesian can still use long-run frequency as part of their beliefs about a phenomena. For a Bayesian, calculating the probability of rolling a 6 can include a prior belief of $\\frac{1}{6}$, based on long-run frequency. However, the final probability weights the data from actually rolling the die with our prior to give us our updated belief about the probability of rolling a 6 on the next go. If the data actually goes against our belief, then our belief shifts, perhaps indicating whether the die is loaded.\n",
    "\n",
    "[^subjective-foot]: Though the view that science must be wholly objective is, itself, a subjective view."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f14eb4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}