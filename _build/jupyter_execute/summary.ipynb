{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c4a964b-a2bc-4d77-b1e8-6928da6b2b73",
   "metadata": {},
   "source": [
    "# Summary\n",
    "In this lesson we have covered a lot of ground on the topic of categorical predictor variables, dummy variable regression and ANOVA. By this point, you should have a good sense of how dummy variables can be used to represent categorical predictor variables within the multiple regression framework. A single dummy coding for two levels of a factor will produce coefficient tests that are identical to the standard $t$-test. Including multiple dummies to code $> 2$ levels produces a model of group means equivalent to a one-way ANOVA. In order to then produce omnibus tests of the factor, we perform a *model comparison* based on comparing residual sums-of-squares. This comparison can then be summarised as an ANOVA table. \n",
    "\n",
    "You should also have a good sense of how this framework can be taken further by including *multiple* categorical predictor variables in the model. This can be achieved in two ways, either through an *additive* or *full factorial* model. The additive model assumes that there are constant row and column effects in the table of cell means. The model comparisons can then be conducted by simply removing each factor individually and assessing the residual sums-of-squares. The full factorial model, by comparison, assumes *non-constant* row and column effects. This is conceptualised in terms of an *interaction* between the two factors, which is parameterised as a mutliplicative effect in the model equation. A significant interaction suggests that the effect of one factor *changes* depending upon the level of another factor. The omnibus interaction effect comes from a simple comparison between the additive and full factorial model. However, the omnibus main effects in the presence of an interaction are more problematic. Although other software defaults to Type III effects in this situation, it is arguable that these are generally nonsensical. Instead, the `Anova()` function from `car` defaults to Type II tests, where the main effects are tested assuming the interaction is 0. As such, we either ignore the main effects when the interaction is significant, or interpret the main effects when the interaction is non-significant.\n",
    "\n",
    "Although we have gone through a lot of information in this lesson, the ANOVA picture is still not fully complete. For factors with $> 2$ levels, a significant main effect or interaction still does not tell us precisely *which* differences are driving the effect. In order to know this, we have to both *visualise* the specific effects and drill-down into each effect to perform further comparisons. This is the domain of *follow-up* or *post-hoc* tests, which will be part of the final lesson next week. We will also explore visualising these effects and will discuss the Type I, II and III distinction further.\n",
    "\n",
    "Make no mistake, we have come very far by this point, but there remains a little further to go. Once we get there, you will have the complete picture of the analysis landscape for a single continuous outcome variable. This means that, no matter the data set, so long as the outcome can be considered a continuous random variable you have all the tools you will ever need to perform a suitable analysis. This is the power of the linear models framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33944ce",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}