{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c4a964b-a2bc-4d77-b1e8-6928da6b2b73",
   "metadata": {},
   "source": [
    "# Summary\n",
    "In this lesson we have covered a lot of ground on the topic of categorical predictor variables, dummy variable regression and ANOVA. By this point, you should have a good sense of how dummy variables can be used to represent categorical predictor variables within the multiple regression framework. A single dummy coding for two levels of a factor will produce coefficient tests that are identical to the standard $t$-test. Including multiple dummies to code $> 2$ levels produces a model of group means equivalent to a one-way ANOVA. More specifically, the dummy variables impose a *constraint* that allows the ANOVA model to be solved. In order to then produce omnibus tests of the factor, we perform a *model comparison* based on comparing residual sums-of-squares. This comparison can then be summarised as an ANOVA table. \n",
    "\n",
    "You should also have a good sense of how this can be taken further by including *multiple* categorical predictor variables in the model. This can be achieved in two ways, either through an *additive* or *full factorial* model. The additive model assumes that there are constant row and column effects in the table of cell means. The model comparisons can then be conducted by simply removing each factor individually and assessing the residual sums-of-squares. The full factorial model, by comparison, assumes *non-constant* row and column effects. This is conceptualised in terms of an *interaction* between the two factors, which is parameterised as a mutliplicative effect in the model equation. A significant interaction suggests that the effect of one factor *changes* depending upon the level of another factor. The omnibus interaction effect comes from a simple comparison between the additive and full factorial model. The omnibus main effects in the presence of an interaction are more problematic. Although other software defaults to Type III effects in this situation, it is arguable that these are generally nonsensical. Instead, the `Anova()` function from `car` defaults to Type II tests, where the main effects are tested assuming the interaction is 0. As such, we either ignore the main effects when the interaction is significant, or interpret the main effects when the interaction is non-significant.\n",
    "\n",
    "Although we have gone through a lot of information in this lesson, the ANOVA picture is still not fully complete. For factors with $> 2$ levels, a significant main effect or interaction still does not tell us precisely *which* differences are driving the effect. In order to know this, we have to drill-down into each effect and perform further comparisons between cell or marginal means. This is the domain of *follow-up* or *post-hoc* tests, which will be part of the final lesson next week. We will also explore visualising these effects and will discuss the Type I, II and III distinction further. We will also discuss the notion of ANCOVA models as linear models containing *both* continuous and categorical variables, as well as more complex interactions that involve *continuous* predictor variables. \n",
    "\n",
    "Make no mistake, we have come very far by this point. However, there remains a little further to go before our picture of the analysis landscape for a single continuous outcome variable is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33944ce",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}