{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6b3fd6-1753-45e7-a920-fccf68cbdcac",
   "metadata": {},
   "source": [
    "# Higher-order ANOVA I: The Additive Model\n",
    "In the previous parts of this lesson, we examined the use of models that only contained a *single* categorical predictor variable. But what about those times when we have *multiple* categorical predictor variables? This is the domain of the *higher-order* ANOVA model, which we will be exploring in both this part and the final part of this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c48fa9",
   "metadata": {},
   "source": [
    "## The Higher-order ANOVA Framework\n",
    "To begin with, it is important that we establish some key concepts about factorial experimental designs. This is because many of these concepts directly inform the mechanics of how an ANOVA model works. This is by design, as the ANOVA was originally developed by Fisher as a principled way of analysing data from factorial experiments. As such, we need to understand some key ideas behind experimental design in order to understand the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518ad36b",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "When we describe ANOVA models, we usually do so with reference to both the number of factors and the number of levels each factor has. For instance, a 1-way ANOVA is so-called because it only has *one* factor. If there were two factors, we would call it a 2-way ANOVA, and so on. If each factor had 2 levels, we could refer to a 2-way ANOVA more specifically as a $2 \\times 2$ ANOVA. Here, each number represents a factor, with the number itself indicating the number of levels. For instance, if the second factor had 3 levels, we would call it a $2 \\times 3$ ANOVA. In a 3-way ANOVA, if each factor had two levels we could call it a $2 \\times 2 \\times 2$ ANOVA. If the second factor had 3 levels and the third had 5 levels it would be a $2 \\times 3 \\times 5$ ANOVA, and so on. Hopefully the pattern is clear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789e85f8",
   "metadata": {},
   "source": [
    "### Means Tables\n",
    "The reason for conceptualising the arrangement of factors and levels in this fashion is that it makes the organisation of the experimental manipulation much clearer. If we think of the experiment in terms of a *table* then the number of *cells* is given by the multiplication of the factor levels. The simplest example is a $2 \\times 2$ ANOVA that can be represented using a table with $2 \\times 2 = 4$ cells. Such a table would look like this:\n",
    "\n",
    "|                       | Factor B: Level 1 | Factor B: Level 2 | \n",
    "|-----------------------|-------------------|-------------------|\n",
    "| **Factor A: Level 1** |                   |                   |\n",
    "| **Factor A: Level 2** |                   |                   |\n",
    "\n",
    "The cells therefore represent the *intersection* of the factor levels. The data from the experiment is collected from each one of these cells and the differences between the cells correspond to the experimental manipulations. Thus, the entire structure of the experiment can be represented this way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ebfdf7",
   "metadata": {},
   "source": [
    "### Cell Means\n",
    "Conceptualising the experiment as a table also makes it clearer how we can *summarise* the effects from the experiment. Given that we have data representative of each cell of the design, the simplest approach is simply to average all the data within each cell to produce a *cell mean*. We can then compare the cell means to assess the magnitude of the effect of each manipulation. The cell means are typically represented by the Greek letter $\\mu$, with subscripts indexing the specific cell. For instance, $\\mu_{12}$ would indicate the mean of the data collected from Level 1 of Factor A and Level 2 of Factor B (row 1 and column 2 of the means table). The complete picture of cell means in a $2 \\times 2$ design is therefore\n",
    "\n",
    "|                       | Factor B: Level 1 | Factor B: Level 2 | \n",
    "|-----------------------|-------------------|-------------------|\n",
    "| **Factor A: Level 1** | $\\mu_{11}$        | $\\mu_{12}$        |\n",
    "| **Factor A: Level 2** | $\\mu_{21}$        | $\\mu_{22}$        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a86bcaa",
   "metadata": {},
   "source": [
    "\n",
    "### Marginal Means\n",
    "Beyond cell means, we can also derive another form of summary known as *marginal means*. These are so-called because they are written in the *margins* of the means table. These types of means concern a specific level of one factor, *averaged-over* the other factors. For instance, there are two cell means that contain Level 1 of Factor A: $\\mu_{11}$ and $\\mu_{12}$. The *marginal mean* for Level 1 of Factor A is therefore $(\\mu_{11} + \\mu_{12})/2 = \\mu_{1.}$. Here, the dot subscript is shorthand for an index that has been averaged-over. This marginal mean is therefore representative of Level 1 of Factor A, *ignoring* Factor B. We can see the complete picture of both *cell means* and *marginal means* below, where $\\mu$ indicates the *grand mean*.\n",
    "\n",
    "|                         | Factor B: Level 1 | Factor B: Level 2 | Marginal Means of A |\n",
    "|-------------------------|-------------------|-------------------|---------------------|\n",
    "| **Factor A: Level 1**   | $\\mu_{11}$        | $\\mu_{12}$        | $\\mu_{1.}$          |\n",
    "| **Factor A: Level 2**   | $\\mu_{21}$        | $\\mu_{22}$        | $\\mu_{2.}$          |\n",
    "| **Marginal Means of B** | $\\mu_{.1}$        | $\\mu_{.2}$        | $\\mu$               |\n",
    "\n",
    "As we go through this section and the next, the relevance of cell means and marginal means will become clearer. For now, it is just important that you understand what these terms mean within the context of a factorial experimental design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313fa1a9",
   "metadata": {},
   "source": [
    "## The Additive Model\n",
    "Now that we have addressed terminology, we can turn to how we include *multiple* factors within a model. Let us start with the simplest approach, which is to simply add another factor to the model equation. In terms of notation, this is a basic extension of what we already know\n",
    "\n",
    "$$\n",
    "y_{ijk} = \\mu + \\alpha_{i} + \\beta_{j} + \\epsilon_{ijk},\n",
    "$$\n",
    "\n",
    "where $\\alpha_{i}$ is the effect associated with Factor A and $\\beta_{j}$ is the effect associated with Factor B. The most basic form of this model would be one that represents a $2 \\times 2$ design, with $i = 1,2$ and $j = 1,2$. We would therefore have 4 cell means and thus 4 unique predicted values formed from:\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{1}\n",
    "    \\mu_{11} &= \\mu + \\alpha_{1} + \\beta_{1} \\\\\n",
    "    \\mu_{21} &= \\mu + \\alpha_{2} + \\beta_{1} \\\\\n",
    "    \\mu_{12} &= \\mu + \\alpha_{1} + \\beta_{2} \\\\\n",
    "    \\mu_{22} &= \\mu + \\alpha_{2} + \\beta_{2}. \n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "Although probably not immediately obvious, this model assumes that the difference between the levels of each factor is the *same*, irrespective of the levels of the other factor. In other words, the model assumes a *constant* difference between the rows or the columns of the means table. For instance, the two differences between the 1st and 2nd levels of Factor A are\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{2}\n",
    "    \\mu_{11} - \\mu_{21} &= \\left(\\mu + \\alpha_{1} + \\beta_{1}\\right) - \\left(\\mu + \\alpha_{2} + \\beta_{1}\\right) &&= \\alpha_{1} - \\alpha_{2} \\quad\\text{(effect of A at level 1 of B)} \\\\\n",
    "    \\mu_{12} - \\mu_{22} &= \\left(\\mu + \\alpha_{1} + \\beta_{2}\\right) - \\left(\\mu + \\alpha_{2} + \\beta_{2}\\right) &&= \\alpha_{1} - \\alpha_{2} \\quad\\text{(effect of A at level 2 of B)}\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "As such, no matter the level of Factor B, the effect of Factor A is always $\\alpha_{1} - \\alpha_{2}$. The same is true across the levels of Factor A, where the effect of Factor B is always $\\beta_{1} - \\beta_{2}$. In other words, this model make the strong assumption that the two factors are entirely *independent* and do not affect each other in any way. The effects of the factors only *add* and so this is known as the assumption of *additivity*.\n",
    "\n",
    "It is important to recognise that additivity is actually a *constraint* on the fitting procedure. By specifying the model in this fashion, either least-squares or maximum likelihood will produce estimated means that adhere to additivity. The estimated cell means will therefore have a constant difference between the rows and the columns of the table. However, if the data does not adhere to additivity, the estimated cell means and the sample cell means will be *different*. The degree to which the model does not fit the actual sample means is therefore indicative of the degree to which the additivity assumption does not hold. We will see this in the example below and will be the starting point for justifying the concept of an *interaction* a little later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514ef5b3",
   "metadata": {},
   "source": [
    "```{admonition} ANOVA Examples\n",
    ":class: tip\n",
    "It can be difficult to conceptualise what an ANOVA model is saying when working in abstract terms such as \"Factor A\" or $\\mu_{12}$. Often, it is useful to have a concrete example to drive the point home. For instance, imagine that Factor A is *depression diagnosis* with two levels: *depressed* and *non-depressed*. Now imagine that Factor B is *anxiety status* with two levels: *high-anxiety* and *low-anxiety*. Our $2 \\times 2$ table of means would be\n",
    "\n",
    "|                   | Depression: Non-depressed   | Depression: Depressed   | \n",
    "|-------------------|-----------------------------|-------------------------|\n",
    "| **Anxiety: Low**  | Low-anxiety, Non-depressed  | Low-anxiety, Depressed  |\n",
    "| **Anxiety: High** | High-anxiety, Non-depressed | High-anxiety, Depressed |\n",
    "\n",
    "<br>\n",
    "\n",
    "Remembering that the additive model assumes a *constant row difference* and a *constant column difference*, this is the same as assuming that the difference between those with and without depression is the same, irrespective of their anxiety (constant *column* difference). Similarly, this is the same as assuming that the difference between those with high and low anxiety is the same, irrespective of whether they are depressed (constant *row* difference). For this particular example, it would seem unlikely that depression and anxiety are two completely independent conditions that do not influence each other in any way.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b204600",
   "metadata": {},
   "source": [
    "### Additive Model Example in `R`\n",
    "As an example, let us expand our `mtcars` analysis with an addition categorical predictor. Within `mtcars` there already exists a factor called `vs` which indicates whether the engine is V-shaped or straight[^engine-foot]. This is already coded as a dummy variable, but we will label it so that it is clearer what it means before turning it into a factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f7e14d2",
   "metadata": {
    "tags": [
     "remove-cell"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "data(mtcars)\n",
    "mtcars$origin <- c('Other','Other','USA','USA','USA','USA','USA','Other','Other','Other',\n",
    "                   'Other','Other','Other','Other','USA','USA','USA','Other','Other',\n",
    "                   'Other','Other','USA','USA','USA','USA','Other','Other','Other',\n",
    "                   'USA','Other','Other','Other')\n",
    "mtcars$origin <- as.factor(mtcars$origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f4dcad",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Straight\" \"V-shaped\"\n"
     ]
    }
   ],
   "source": [
    "vs.lab <- rep(\"\",length(mtcars$vs)) \n",
    "vs.lab[mtcars$vs == 0] <- \"V-shaped\"\n",
    "vs.lab[mtcars$vs == 1] <- \"Straight\"\n",
    "\n",
    "mtcars$vs <- as.factor(vs.lab)\n",
    "print(levels(mtcars$vs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600075be",
   "metadata": {},
   "source": [
    "We will also work with the simpler version of `origin`, where we only had 2 levels: `USA` and `Other`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5855e695",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Other\" \"USA\"  \n"
     ]
    }
   ],
   "source": [
    "print(levels(mtcars$origin))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3958f6",
   "metadata": {},
   "source": [
    "Our table of means is therefore\n",
    "\n",
    "|                   | VS: Straight | VS: V-shaped | \n",
    "|-------------------|--------------|--------------|\n",
    "| **Origin: Other** | $\\mu_{11}$   | $\\mu_{12}$   |\n",
    "| **Origin: USA**   | $\\mu_{21}$   | $\\mu_{22}$   |\n",
    "\n",
    "We can now examine how `R` has coded both `vs` and `origin` as dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7d80faf",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      USA\n",
      "Other   0\n",
      "USA     1\n",
      "         V-shaped\n",
      "Straight        0\n",
      "V-shaped        1\n"
     ]
    }
   ],
   "source": [
    "print(contrasts(mtcars$origin))\n",
    "print(contrasts(mtcars$vs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16950d52",
   "metadata": {},
   "source": [
    "So there are now 4 unique combinations of dummy values that lead to the 4 cell means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49876b0c",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{alignat*}{2}\n",
    "    \\mu_{11} &= \\beta_{0} + (\\beta_{1} \\times \\mathbf{0}) + (\\beta_{2} \\times \\mathbf{0}) = \\beta_{0} &&\\quad\\texttt{(Other,Straight)} \\\\\n",
    "    \\mu_{21} &= \\beta_{0} + (\\beta_{1} \\times \\mathbf{1}) + (\\beta_{2} \\times \\mathbf{0}) = \\beta_{0} + \\beta_{1} &&\\quad\\texttt{(USA,Straight)} \\\\\n",
    "    \\mu_{12} &= \\beta_{0} + (\\beta_{1} \\times \\mathbf{0}) + (\\beta_{2} \\times \\mathbf{1}) = \\beta_{0} + \\beta_{2} &&\\quad\\texttt{(Other,V-shaped)} \\\\\n",
    "    \\mu_{22} &= \\beta_{0} + \\underbrace{(\\beta_{1} \\times \\mathbf{1})}_{\\texttt{origin}} + \\underbrace{(\\beta_{2} \\times \\mathbf{1})}_{\\texttt{vs}} = \\beta_{0} + \\beta_{1} + \\beta_{2} &&\\quad\\texttt{(USA,V-shaped)}\\\\\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "Using the same logic we used in the previous part of this lesson, we can work out that\n",
    "\n",
    "| Parameter   | Meaning                                               | Interpretation           |\n",
    "|-------------|-------------------------------------------------------|--------------------------|\n",
    "| $\\beta_{0}$ | Mean of `(Other,Straight)` cell                       | Reference cell           |\n",
    "| $\\beta_{1}$ | Mean difference `(USA,Straight) - (Other,Straight)`   | Constant *column* effect |\n",
    "| $\\beta_{2}$ | Mean difference `(Other,V-shaped) - (Other,Straight)` | Constant *row* effect    |\n",
    "\n",
    "This also helps make sense of how the model prediction works. Thinking of the means table, if we start at the reference cell ($\\mu_{11} = \\beta_{0}$) and then add the *row effect* ($\\beta_{1}$) we move *down* a row and end up at $\\mu_{21}$. Instead, if we start at the reference cell ($\\mu_{11} = \\beta_{0}$) and then add the *column effect* ($\\beta_{2}$) we move *across* a column and end up at $\\mu_{12}$. Finally, if we start at the reference cell ($\\mu_{11} = \\beta_{0}$) and then add the *row effect* ($\\beta_{1}$) and the *column effect* ($\\beta_{2}$) we move down a row and across a column and end up at $\\mu_{22}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77b2363",
   "metadata": {},
   "source": [
    "\n",
    "```{warning}\n",
    "The ability to parameterise the cell means in this way only works because of the *additive assumptions* about constant row and column effects. If the cell means did not adhere to this, you could not move up/down or left/right across the table of means freely, because the row/column effects would change depending upon which cell you started in. Under the additive model, it does not matter where you start because the differences are always the same.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c13a3c",
   "metadata": {},
   "source": [
    "Given all this, we can now see how `R` fits this model and check that it aligns with our understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3450611",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call:\n",
      "lm(formula = mpg ~ origin + vs, data = mtcars)\n",
      "\n",
      "Residuals:\n",
      "    Min      1Q  Median      3Q     Max \n",
      "-7.7035 -3.2079  0.1795  1.9298  8.3965 \n",
      "\n",
      "Coefficients:\n",
      "            Estimate Std. Error t value Pr(>|t|)    \n",
      "(Intercept)   25.504      1.157  22.036  < 2e-16 ***\n",
      "originUSA     -4.416      1.587  -2.783  0.00939 ** \n",
      "vsV-shaped    -6.433      1.571  -4.094  0.00031 ***\n",
      "---\n",
      "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
      "\n",
      "Residual standard error: 4.139 on 29 degrees of freedom\n",
      "Multiple R-squared:  0.5588,\tAdjusted R-squared:  0.5283 \n",
      "F-statistic: 18.36 on 2 and 29 DF,  p-value: 7.044e-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "add.mod <- lm(mpg ~ origin + vs, data=mtcars)\n",
    "print(summary(add.mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c91558",
   "metadata": {},
   "source": [
    "These estimates certainly look like the values we determined in the table above, with the intercept giving us a *cell mean* and the two slope parameters giving us *mean differences*. Based on this, we can construct the estimated cell means from the estimated parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74c4e44c",
   "metadata": {
    "tags": [
     "hide-input"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            origin.other origin.USA\n",
      "vs.straight     25.50350   21.08716\n",
      "vs.vshaped      19.07019   14.65385\n"
     ]
    }
   ],
   "source": [
    "beta <- coef(add.mod)\n",
    "\n",
    "# fitted cell means\n",
    "mu.other.str <- beta[1]\n",
    "mu.USA.str   <- beta[1] + beta[2]\n",
    "mu.other.v   <- beta[1] + beta[3]\n",
    "mu.USA.v     <- beta[1] + beta[2] + beta[3]\n",
    "\n",
    "# means table\n",
    "est.means.tbl <- data.frame(\"origin.other\"=c(mu.other.str,mu.other.v),\n",
    "                            \"origin.USA\"  =c(mu.USA.str,  mu.USA.v),\n",
    "                            row.names=c(\"vs.straight\",\"vs.vshaped\"))\n",
    "\n",
    "print(est.means.tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fbe319",
   "metadata": {},
   "source": [
    "As expected, these fitted cell means have a constant difference between each row[^unname-foot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "668b8171",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 6.433314\n",
      "[1] 6.433314\n"
     ]
    }
   ],
   "source": [
    "print(unname(mu.other.str - mu.other.v))\n",
    "print(unname(mu.USA.str   - mu.USA.v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2c57b3",
   "metadata": {},
   "source": [
    "and a constant difference between each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c37e47af",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 4.416336\n",
      "[1] 4.416336\n"
     ]
    }
   ],
   "source": [
    "print(unname(mu.other.str - mu.USA.str))\n",
    "print(unname(mu.other.v   - mu.USA.v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c700918",
   "metadata": {},
   "source": [
    "However, these estimated means do not match the actual sample means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44225415",
   "metadata": {
    "tags": [
     "hide-input"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample means:\n",
      "            origin.other origin.USA\n",
      "vs.straight     25.59091   20.76667\n",
      "vs.vshaped      18.95000   14.75000\n",
      "\n",
      "Estimated means:\n",
      "            origin.other origin.USA\n",
      "vs.straight     25.50350   21.08716\n",
      "vs.vshaped      19.07019   14.65385\n"
     ]
    }
   ],
   "source": [
    "# sample cell means\n",
    "mu.other.str <- mean(mtcars$mpg[mtcars$origin == \"Other\" & mtcars$vs == \"Straight\"])\n",
    "mu.USA.str   <- mean(mtcars$mpg[mtcars$origin == \"USA\"   & mtcars$vs == \"Straight\"])\n",
    "mu.other.v   <- mean(mtcars$mpg[mtcars$origin == \"Other\" & mtcars$vs == \"V-shaped\"])\n",
    "mu.USA.v     <- mean(mtcars$mpg[mtcars$origin == \"USA\"   & mtcars$vs == \"V-shaped\"])\n",
    "\n",
    "# means table\n",
    "samp.means.tbl <- data.frame(\"origin.other\"=c(mu.other.str, mu.other.v),\n",
    "                             \"origin.USA\"  =c(mu.USA.str,   mu.USA.v),  \n",
    "                             row.names=c(\"vs.straight\",\"vs.vshaped\"))\n",
    "\n",
    "cat(\"Sample means:\\n\") # allows more control over printing than print()\n",
    "print(samp.means.tbl)\n",
    "cat(\"\\nEstimated means:\\n\")\n",
    "print(est.means.tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750016ab",
   "metadata": {},
   "source": [
    "Now, the estimates and the true means are not too far off, so there are a few possibilities here. Firstly, it could be that the assumptions of the additive model do not hold in this example. Alternatively, it is possible that these effects are truly additive in the population and any deviation is simply sampling noise. Or, it could be that the degree to which these two factors influence each other is very minor and that we could treat these factors as additive for the sake of simplicity. We will come back to this when we explore the Full Factorial model in the next part of the lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2420f97b",
   "metadata": {},
   "source": [
    "### Main Effects\n",
    "In the previous part of this lesson, we discussed the concept of an *omnibus test* as an overall reflection of whether any of the mean differences across the levels of a categorical predictor are non-zero. This idea carries-over into higher-order ANOVA models. The main difference is that our null models are rarely intercept-only. Instead, we include all other factors in the null model *except* the factor of interest. The logic is that the additional factors may relate to the outcome variable and thus should be included in the null model. This is so that the error is as accurate as possible and does not accidentally contain any sources of systematic variation which would make it larger than it should be. Another way of thinking about this is that the omnibus null for one variable is not based on assuming that there are *no relationships with any other variables*. As such, the null model simply involves removing a *single variable*, not all of them[^reganova-foot]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7f6f1f",
   "metadata": {},
   "source": [
    "```{admonition} Main Effects Definition\n",
    ":class: tip\n",
    "Within the context of an ANOVA, the omnibus effect of a single categorical predictor is known as a *main effect*. Under additivity, this reflects the omnibus effect of a single factor, ignoring all other factors in the model. For instance, what is the effect of `origin`, irrespective of whether an engine is `straight` or `v-shaped`? Or, what is the effect of *depression*, irrespective of whether someone is *highly-anxious* or not? Main effects *require* additivity to be interpretable and are based on comparing *marginal means* for the factor of interest.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aacb07e",
   "metadata": {},
   "source": [
    "Based on everything above, the *main effect* of Factor A is equivalent to the following model comparison\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{2}\n",
    "    \\mathcal{M}_{0} &: y_{jk}  &&= \\mu + \\beta_{j} + \\epsilon_{jk} \\\\\n",
    "    \\mathcal{M}_{1} &: y_{ijk} &&= \\mu + \\alpha_{i} + \\beta_{j} + \\epsilon_{ijk},\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "and the main effect of Factor B is equivalent to the following model comparison\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{2}\n",
    "    \\mathcal{M}_{0} &: y_{ik}  &&= \\mu + \\alpha_{i} + \\epsilon_{ik} \\\\\n",
    "    \\mathcal{M}_{1} &: y_{ijk} &&= \\mu + \\alpha_{i} + \\beta_{j} + \\epsilon_{ijk}.\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "In each case, we simply remove the terms associated with the factor of interest and see whether the change in the residual sums-of-squares is large relative to the error. This is then equivalent to comparing all the *marginal means* associated with a single factor. In the case of a $2 \\times 2$ design, the omnibus null for these factors would be\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{1}\n",
    "    \\mathcal{H}_{0} &: \\mu_{1.} = \\mu_{2.} \\quad\\text{(Main effect of Factor A)} \\\\\n",
    "    \\mathcal{H}_{0} &: \\mu_{.1} = \\mu_{.2} \\quad\\text{(Main effect of Factor B)}. \\\\\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "Although both these hypotheses only involve two means, they still remain dissimilar to a $t$-test by virtue of taking the *other factor* into account. This is not something you can do with a simple $t$-test. Of course, when dealing with factors that contain $> 2$ levels, the need for an omnibus test becomes even clearer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcba3624",
   "metadata": {},
   "source": [
    "#### Main Effects in `R`\n",
    "As we saw previously, we can form omnibus tests from explicit model comparisons using the `anova()` function. Using this method, the main effect of `origin` would be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "782fc3bb",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of Variance Table\n",
      "\n",
      "Model 1: mpg ~ vs\n",
      "Model 2: mpg ~ origin + vs\n",
      "  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   \n",
      "1     30 629.52                                \n",
      "2     29 496.86  1    132.66 7.7428 0.009386 **\n",
      "---\n",
      "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n"
     ]
    }
   ],
   "source": [
    "null.mod <- lm(mpg ~ vs,          data=mtcars)\n",
    "full.mod <- lm(mpg ~ origin + vs, data=mtcars)\n",
    "\n",
    "print(anova(null.mod,full.mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291846e1",
   "metadata": {},
   "source": [
    "and the main effect of `vs` would be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d482c4de",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of Variance Table\n",
      "\n",
      "Model 1: mpg ~ origin\n",
      "Model 2: mpg ~ origin + vs\n",
      "  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n",
      "1     30 784.06                                  \n",
      "2     29 496.86  1     287.2 16.763 0.0003096 ***\n",
      "---\n",
      "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n"
     ]
    }
   ],
   "source": [
    "null.mod <- lm(mpg ~ origin,      data=mtcars)\n",
    "full.mod <- lm(mpg ~ origin + vs, data=mtcars)\n",
    "\n",
    "print(anova(null.mod,full.mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054947b2",
   "metadata": {},
   "source": [
    "Or, more practically, we can simply give the full model to the `Anova()` function from the `car` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "974f11f2",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: carData\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anova Table (Type II tests)\n",
      "\n",
      "Response: mpg\n",
      "          Sum Sq Df F value    Pr(>F)    \n",
      "origin    132.66  1  7.7428 0.0093864 ** \n",
      "vs        287.20  1 16.7628 0.0003096 ***\n",
      "Residuals 496.86 29                      \n",
      "---\n",
      "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n"
     ]
    }
   ],
   "source": [
    "library(car)\n",
    "print(Anova(add.mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c051e2",
   "metadata": {},
   "source": [
    "Compare the `Anova()` output with the outputs from the model comparisons to reassure yourself that these are *identical*. \n",
    "\n",
    "Based on this result, there are significant differences in `mpg` between the levels of both `origin` and `vs`, assuming the two factors have independent influences on `mpg`. In context, this is assuming that there is nothing about where the car is manufactured that influences the shape of the engine, nor anything about the shape of the engine that influences where it was manufactured. Based on this, we would conclude that *both* the engine shape and the country of origin independently influence MPG.\n",
    "\n",
    "We can also generate Cohen's $f$ here as well, as shown below. Here we can see that `vs` is a larger effect than `origin`, but both are above Cohen's recommendation of $f = 0.4$ for a *large* effect. That being said, the uncertainty for `origin` is much higher, as the lower-bound of the CI is between *small* and *medium* (though closer to the *medium* boundary of $f = 0.25$). So `vs` would seem the stronger result, though both remain notable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a9cd918",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m# Effect Size for ANOVA (Type II)\u001b[39m\n",
      "\n",
      "Parameter | Cohen's f (partial) |      95% CI\n",
      "---------------------------------------------\n",
      "origin    |                0.52 | [0.19, Inf]\n",
      "vs        |                0.76 | [0.41, Inf]\n",
      "\u001b[36m\n",
      "- One-sided CIs: upper bound fixed at [Inf].\u001b[39m"
     ]
    }
   ],
   "source": [
    "library(effectsize)\n",
    "print(cohens_f(Anova(add.mod)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debb127b",
   "metadata": {},
   "source": [
    "`````{topic} What do you now know?\n",
    "In this section, we have explored higher-order ANOVA models in terms of a simple additive model. After reading this section, you should have a good sense of:\n",
    "\n",
    "- The difference between *cell means*, *marginal means* and how these correspond to a factorial experimental design.\n",
    "- The concept of the *means table* as a way of summarising an experimental manipulation.\n",
    "- The model form of a simple 2-way ANOVA and how this differs from a 1-way ANOVA.\n",
    "- The idea of assuming *additivity* in terms of a constant row and column difference between the cell means.\n",
    "- How multiple factors can be coded as dummy variables and what this implies about the meaning of the parameters.\n",
    "- What a *main effect* is and how it can be constructed using model comparisons.\n",
    "\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdf905e",
   "metadata": {},
   "source": [
    "[^engine-foot]: Not that it really matters for understanding an ANOVA, but a straight engine has cylinders in a straight line, while a V-shaped engine has two banks of cylinders arranged in a \"V\". V-shaped engines tend to be more compact and powerful. Straight engines are often more balanced, whereas V engines are shorter and can be better for performance-focused applications.\n",
    "\n",
    "[^round-foot]: The use of the `round()` function here is only because, due to the way that `R` solves for the model parameters, there can be some differences between estimates deep into the decimal places. This can fool the `unique()` function, which will return values that are apparently identical. This is simply an issue with numeric precision. As such, it is always worth using `round()` in combination with `unique()` to prevent this.\n",
    "\n",
    "[^unname-foot]: All the `unname()` function is doing here is preventing `R` from printing any labels alongside these results and making them less clear. Sometimes, values carry labels with them across different calculations and you get some odd names, like `(Intercept)` being printed above the calculations. This stops that from happening. You can remove it, if you like, and see what happens.\n",
    "\n",
    "[^reganova-foot]: If we want to remove *everything*, we can always use the regression ANOVA report at the bottom of the summary table, as mentioned at the end of the previous part of the lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043cb46e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}