{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6b3fd6-1753-45e7-a920-fccf68cbdcac",
   "metadata": {},
   "source": [
    "# Higher-order ANOVA Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c48fa9",
   "metadata": {},
   "source": [
    "## The Higher-order ANOVA Framework\n",
    "\n",
    "### Terminology and Mean Tables\n",
    "\n",
    "### Cell Means and Marginal Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1f7571",
   "metadata": {},
   "source": [
    "\n",
    "```{admonition} Higher-order terminology\n",
    ":class: tip\n",
    "... For instance, the most basic higher-order ANOVA is the two-way ANOVA, that contains two factors. IF each factor had two levels, we would write this as a $2 \\times 2$ ANOVA. If the second factor had three levels, we would call it a $2 \\times 3$ ANOVA. If there were *three* factors, we would have a three-way ANOVA. If each factor had two levels we could call it a $2 \\times 2 \\times 2$ ANOVA, and so on.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313fa1a9",
   "metadata": {},
   "source": [
    "## The Additive Model\n",
    "Let us start with the simplest approach, which is to just add another factor to the model. In terms of notation, this is a basic extension of what we already known\n",
    "\n",
    "$$\n",
    "y_{ijk} = \\mu + \\alpha_{i} + \\beta_{j} + \\epsilon_{ijk},\n",
    "$$\n",
    "\n",
    "where $\\alpha_{i}$ is the effect associated with Factor A and $\\beta_{j}$ is the effect associated with Factor B. The most basic form of this model would be one that represents a $2 \\times 2$ design, with $i = 1,2$ and $j = 1,2$.\n",
    "\n",
    "It is important to recognise at this point the assumptions that this model makes. If we stick with a $2 \\times 2$ design then we have 4 cell means \n",
    "\n",
    "|                       | Factor B: Level 1 | Factor B: Level 2 | \n",
    "|-----------------------|-------------------|-------------------|\n",
    "| **Factor A: Level 1** | $\\mu_{11}$        | $\\mu_{12}$        |\n",
    "| **Factor A: Level 2** | $\\mu_{21}$        | $\\mu_{22}$        |\n",
    "\n",
    "and thus 4 unique predicted values formed from:\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{1}\n",
    "    \\mu_{11} &= \\mu + \\alpha_{1} + \\beta_{1} \\\\\n",
    "    \\mu_{21} &= \\mu + \\alpha_{2} + \\beta_{1} \\\\\n",
    "    \\mu_{12} &= \\mu + \\alpha_{1} + \\beta_{2} \\\\\n",
    "    \\mu_{22} &= \\mu + \\alpha_{2} + \\beta_{2}. \n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "Although probably not immediately obvious, this model assumes that the difference between the levels of each factor is the *same*, irrespective of the levels of the other factor. In other words, the model assumes a *constant* difference between the rows or the columns of the means table. For instance, the two differences between the 1st and 2nd levels of Factor A are\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{2}\n",
    "    \\mu_{11} - \\mu_{21} &= \\left(\\mu + \\alpha_{1} + \\beta_{1}\\right) - \\left(\\mu + \\alpha_{2} + \\beta_{1}\\right) &&= \\alpha_{1} - \\alpha_{2} \\\\\n",
    "    \\mu_{12} - \\mu_{22} &= \\left(\\mu + \\alpha_{1} + \\beta_{2}\\right) - \\left(\\mu + \\alpha_{2} + \\beta_{2}\\right) &&= \\alpha_{1} - \\alpha_{2}\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "As such, no matter the level of Factor B, $\\alpha_{1} - \\alpha_{2}$ is always the same. The same is true across the levels of Factor A, where $\\beta_{1} - \\beta_{2}$ is always the same. In other words, this model make the strong assumption that the two factors are entirely *independant* and do not affect each other in any way. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee651a01",
   "metadata": {},
   "source": [
    "It is important to recognise that this assumption of additivity is actually a *constraint* on the fitting procedure. By specifying the model in this fashion, either least-squares or maximum likelihood will produce estimated means that adhere to additivity. The estimated cell means will therefore have a constant difference between the rows and the columns of the table. However, if this assumption is not true, the estimated cell means and the sample cell means will be *different*. The degree to which the model does not fit the actual sample means is therefore indicative of the degree to which the additivity assumption does not hold. We will see this in the example below and will be the starting point for justifying the concept of an *interaction* a little later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514ef5b3",
   "metadata": {},
   "source": [
    "```{admonition} Grounding ANOVA Examples\n",
    ":class: tip\n",
    "It can be difficult to conceptualise what an ANOVA model is saying when working in abstract terms such as \"Factor A\" or $\\mu_{12}$. Often, it is useful to have a concrete example to drive the point home. For instance, imagine that Factor A is *depression diagnosis* with two levels: *depressed* and *non-depressed*. Now imagine that Factor B is *anxiety status* with two levels: *high-anxiety* and *low-anxiety*. Our $2 \\times 2$ table of means would be\n",
    "\n",
    "|                   | Depression: Non-depressed   | Depression: Depressed   | \n",
    "|-------------------|-----------------------------|-------------------------|\n",
    "| **Anxiety: Low**  | Low-anxiety, Non-depressed  | Low-anxiety, Depressed  |\n",
    "| **Anxiety: High** | High-anxiety, Non-depressed | High-anxiety, Depressed |\n",
    "\n",
    "\n",
    "Remembering that the additive model assumes a *constant row difference* and a *constant column difference*, this is the same as assuming that the difference between those with and without depression is the same, irrespective of their anxiety (constant *column* difference). Similarly, this is the same as assuming that the difference between those with high and low anxiety is the same, irrespective of whether they are depressed (constant *row* difference). Of course, this depends entirely on what our outcome measure actually is. However, in the real world, it would seem unlikely that depression and anxiety are two completely independant conditions that do not influence each other in any way.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b204600",
   "metadata": {},
   "source": [
    "### Additive Model Example in `R`\n",
    "As an example, let us expand our `mtcars` analysis with an addition categorical predictor. Within `mtcars` there already exists a factor called `vs` which indicates whether the engine is V-shaped or straight[^engine-foot]. This is already coded as a dummy variable, but we will label it so that it is clearer what it means before turning it into a factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f7e14d2",
   "metadata": {
    "tags": [
     "remove-cell"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "data(mtcars)\n",
    "mtcars$origin <- c('Other','Other','USA','USA','USA','USA','USA','Other','Other','Other',\n",
    "                   'Other','Other','Other','Other','USA','USA','USA','Other','Other',\n",
    "                   'Other','Other','USA','USA','USA','USA','Other','Other','Other',\n",
    "                   'USA','Other','Other','Other')\n",
    "mtcars$origin <- as.factor(mtcars$origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37f4dcad",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Straight\" \"V-shaped\"\n"
     ]
    }
   ],
   "source": [
    "vs.lab <- rep(\"\",length(mtcars$vs)) \n",
    "vs.lab[mtcars$vs == 0] <- \"V-shaped\"\n",
    "vs.lab[mtcars$vs == 1] <- \"Straight\"\n",
    "\n",
    "mtcars$vs <- as.factor(vs.lab)\n",
    "print(levels(mtcars$vs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600075be",
   "metadata": {},
   "source": [
    "We will also work with the simpler version of `origin`, where we only had 2 levels: `USA` and `Other`. Our table of means is therefore\n",
    "\n",
    "|                   | VS: Straight | VS: V-shaped | \n",
    "|-------------------|--------------|--------------|\n",
    "| **Origin: Other** | $\\mu_{11}$   | $\\mu_{12}$   |\n",
    "| **Origin: USA**   | $\\mu_{21}$   | $\\mu_{22}$   |\n",
    "\n",
    "So, $\\mu_{11}$ will indicate `(Other,Straight)`, $\\mu_{21}$ will indicate `(USA,Straight)` and so on. \n",
    "\n",
    "We can now examine how `R` has coded both `vs` and `origin` as dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7d80faf",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      USA\n",
      "Other   0\n",
      "USA     1\n",
      "         V-shaped\n",
      "Straight        0\n",
      "V-shaped        1\n"
     ]
    }
   ],
   "source": [
    "print(contrasts(mtcars$origin))\n",
    "print(contrasts(mtcars$vs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16950d52",
   "metadata": {},
   "source": [
    "So there are now 4 unique combinations of dummy values that lead to the 4 cell means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49876b0c",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{alignat*}{2}\n",
    "    \\mu_{11} &= \\beta_{0} + (\\beta_{1} \\times \\mathbf{0}) + (\\beta_{2} \\times \\mathbf{0}) = \\beta_{0} &&\\quad\\texttt{(Other,Straight)} \\\\\n",
    "    \\mu_{21} &= \\beta_{0} + (\\beta_{1} \\times \\mathbf{1}) + (\\beta_{2} \\times \\mathbf{0}) = \\beta_{0} + \\beta_{1} &&\\quad\\texttt{(USA,Straight)} \\\\\n",
    "    \\mu_{12} &= \\beta_{0} + (\\beta_{1} \\times \\mathbf{0}) + (\\beta_{2} \\times \\mathbf{1}) = \\beta_{0} + \\beta_{2} &&\\quad\\texttt{(Other,V-shaped)} \\\\\n",
    "    \\mu_{22} &= \\beta_{0} + \\underbrace{(\\beta_{1} \\times \\mathbf{1})}_{\\texttt{origin}} + \\underbrace{(\\beta_{2} \\times \\mathbf{1})}_{\\texttt{vs}} = \\beta_{0} + \\beta_{1} + \\beta_{2} &&\\quad\\texttt{(USA,V-shaped)}\\\\\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "Based on this, we can work out that\n",
    "\n",
    "| Parameter   | Meaning                                               | Interpretation           |\n",
    "|-------------|-------------------------------------------------------|--------------------------|\n",
    "| $\\beta_{0}$ | Mean of `(Other,Straight)` cell                       | Reference cell           |\n",
    "| $\\beta_{1}$ | Mean difference `(USA,Straight) - (Other,Straight)`   | Constant *column* effect |\n",
    "| $\\beta_{2}$ | Mean difference `(Other,V-shaped) - (Other,Straight)` | Constant *row* effect    |\n",
    "\n",
    "This also helps make sense of how the model prediction works. If we start at the reference cell ($\\beta_{0} = \\mu_{11}$) and then add the *row effect* ($\\beta_{1}$) we move *down* a row and end up at $\\mu_{21}$. Instead, if we start at the reference cell ($\\beta_{0} = \\mu_{11}$) and then add the *column effect* ($\\beta_{2}$) we move *across* a column and end up at $\\mu_{12}$. Finally, if we start at the reference cell ($\\beta_{0} = \\mu_{11}$) and then add the *row effect* ($\\beta_{1}$) and the *column effect* ($\\beta_{2}$) we move down a row and across a column and end up at $\\mu_{22}$. \n",
    "\n",
    "```{warning}\n",
    "Note that the ability to determine the cell means in this way only works because of the *additive assumptions* about contant row and column effects. If the cell means did not adhere to this, then you could not move up/down or left/right across the table of means freely, because the row/column effects would change depending upon which cell you started in. Under the additive model, it does not matter where you start because the differences are always the same.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c13a3c",
   "metadata": {},
   "source": [
    "Given all this, we can now see how `R` fits this model and check that it aligns with our understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3450611",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call:\n",
      "lm(formula = mpg ~ origin + vs, data = mtcars)\n",
      "\n",
      "Residuals:\n",
      "    Min      1Q  Median      3Q     Max \n",
      "-7.7035 -3.2079  0.1795  1.9298  8.3965 \n",
      "\n",
      "Coefficients:\n",
      "            Estimate Std. Error t value Pr(>|t|)    \n",
      "(Intercept)   25.504      1.157  22.036  < 2e-16 ***\n",
      "originUSA     -4.416      1.587  -2.783  0.00939 ** \n",
      "vsV-shaped    -6.433      1.571  -4.094  0.00031 ***\n",
      "---\n",
      "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
      "\n",
      "Residual standard error: 4.139 on 29 degrees of freedom\n",
      "Multiple R-squared:  0.5588,\tAdjusted R-squared:  0.5283 \n",
      "F-statistic: 18.36 on 2 and 29 DF,  p-value: 7.044e-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "add.mod <- lm(mpg ~ origin + vs, data=mtcars)\n",
    "print(summary(add.mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c91558",
   "metadata": {},
   "source": [
    "Based on this, we can construct the estimated cell means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74c4e44c",
   "metadata": {
    "tags": [
     "hide-input"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            origin.other origin.USA\n",
      "vs.straight     25.50350   21.08716\n",
      "vs.vshaped      19.07019   14.65385\n"
     ]
    }
   ],
   "source": [
    "beta <- coef(add.mod)\n",
    "\n",
    "# fitted cell means\n",
    "mu.other.straight <- beta[1]\n",
    "mu.USA.straight   <- beta[1] + beta[2]\n",
    "mu.other.vshaped  <- beta[1] + beta[3]\n",
    "mu.USA.vshaped    <- beta[1] + beta[2] + beta[3]\n",
    "\n",
    "# means table\n",
    "means.tbl <- data.frame(\"origin.other\"=c(mu.other.straight,mu.other.vshaped),\n",
    "                        \"origin.USA\"  =c(mu.USA.straight,  mu.USA.vshaped),\n",
    "                        row.names=c(\"vs.straight\",\"vs.vshaped\"))\n",
    "\n",
    "print(means.tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fbe319",
   "metadata": {},
   "source": [
    "As expected, these fitted values have a constant difference between each row[^unname-foot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "668b8171",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 6.433314\n",
      "[1] 6.433314\n"
     ]
    }
   ],
   "source": [
    "print(unname(mu.other.straight - mu.other.vshaped))\n",
    "print(unname(mu.USA.straight   - mu.USA.vshaped))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2c57b3",
   "metadata": {},
   "source": [
    "and a constant difference between each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c37e47af",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 4.416336\n",
      "[1] 4.416336\n"
     ]
    }
   ],
   "source": [
    "print(unname(mu.other.straight - mu.USA.straight))\n",
    "print(unname(mu.other.vshaped  - mu.USA.vshaped))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c700918",
   "metadata": {},
   "source": [
    "Unfortunately, these estimated means do not match the actual sample means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44225415",
   "metadata": {
    "tags": [
     "hide-input"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            origin.other origin.USA\n",
      "vs.straight          NaN        NaN\n",
      "vs.vshaped           NaN        NaN\n"
     ]
    }
   ],
   "source": [
    "# sample cell means\n",
    "mu.other.straight <- mean(mtcars$mpg[mtcars$origin == \"Other\" & mtcars$vs == \"straight\"])\n",
    "mu.USA.straight   <- mean(mtcars$mpg[mtcars$origin == \"USA\"   & mtcars$vs == \"straight\"])\n",
    "mu.other.vshaped  <- mean(mtcars$mpg[mtcars$origin == \"Other\" & mtcars$vs == \"v-shaped\"])\n",
    "mu.USA.vshaped    <- mean(mtcars$mpg[mtcars$origin == \"USA\"   & mtcars$vs == \"v-shaped\"])\n",
    "\n",
    "# means table\n",
    "means.tbl <- data.frame(\"origin.other\"=c(mu.other.straight,mu.other.vshaped),\n",
    "                        \"origin.USA\"  =c(mu.USA.straight,  mu.USA.vshaped),\n",
    "                        row.names=c(\"vs.straight\",\"vs.vshaped\"))\n",
    "\n",
    "print(means.tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750016ab",
   "metadata": {},
   "source": [
    "Now, these are not too far off, so there are a few possibilities here. Firstly, it could be that the assumptions of the additive model do not hold in this example. Alternatively, it is possible that these effects are truly additive in the population and any deviation is simply sampling noise. Or, it could be that the degree to which these two factors influence each other is very minor and that we could treat these factors as additive for the sake of simplicity. We will come back to all this when we explore the Full Factorial model below. Firstly, we need to discuss the ANOVA omnibus main effects in a $2 \\times 2$ design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2420f97b",
   "metadata": {},
   "source": [
    "### Main Effects\n",
    "In the previous part of this lesson, we discussed the concept of an *omnibus test* as an overall reflection of whether any of the mean differences across the levels of a categorical predictor are large. This idea carries-over into higher-order ANOVA models, with the main difference being that our null models are rarely intercept-only. Instead, we include all other factors in the null model *except* the factor of interest. The logic is that the additional factors may relate to the outcome variable and thus should be included in the null model. This is so that the error is as accurate as possible and does not accidentally contain any sources of systematic variation which would make it larger than it should be. Another way of thinking about this is that the omnibus null for one variable is not based on assuming that there are *no relationships with any other variables*. As such, the null model simply involves removing a *single variable*, not all of them[^reganova-foot]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7f6f1f",
   "metadata": {},
   "source": [
    "```{admonition} Defining Main Effects\n",
    ":class: tip\n",
    "Within the context of an ANOVA, the omnibus effect of a single categorical predictor is known as a *main effect*. Under additivity, this reflects the omnibus effect of a single factor, ignoring all other factors in the model. For instance, what is the effect of `origin`, irrespective of whether an engine is `straight` or `v-sahped`? Or, what is the effect of *depression*, irrespective of whether someone is *highly-anxious* or not? Main effects *require* additivity to be interpretable and are based on comparing *marginal means* for the factor of interest.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aacb07e",
   "metadata": {},
   "source": [
    "Based on everything above, the *main effect* of Factor A is equivalent to the following model comparison\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{2}\n",
    "    \\mathcal{M}_{0} &: y_{jk}  &&= \\mu + \\beta_{j} + \\epsilon_{jk} \\\\\n",
    "    \\mathcal{M}_{1} &: y_{ijk} &&= \\mu + \\alpha_{i} + \\beta_{j} + \\epsilon_{ijk},\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "and the main effect of Factor B is equivalent to the following model comparison\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{2}\n",
    "    \\mathcal{M}_{0} &: y_{ik}  &&= \\mu + \\alpha_{i} + \\epsilon_{ik} \\\\\n",
    "    \\mathcal{M}_{1} &: y_{ijk} &&= \\mu + \\alpha_{i} + \\beta_{j} + \\epsilon_{ijk}.\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "In each case, we simply remove the terms associated with the factor of interest and see whether the change in the residual sums-of-squares is large relative to the error. This is then equivalent to comparing all the *marginal means* associated with a single factor. In the case of a $2 \\times 2$ design, the omnibus null for these factors would be\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{1}\n",
    "    \\mathcal{H}_{0}^{\\text{A}} &: \\mu_{1.} = \\mu_{2.} \\\\\n",
    "    \\mathcal{H}_{0}^{\\text{B}} &: \\mu_{.1} = \\mu_{.2} \\\\\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "Under additivity, the differences between the marginal means are *identical* to the constant row or column difference and thus directly capture the overall differences between the levels of a single factor, irrespective or the levels of the other factors in the model. When additivity does not hold, this logic breaks down as the marginal means may not longer capture any effects that reflect the actual data. We will see this later when we discuss *interactions*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcba3624",
   "metadata": {},
   "source": [
    " Main Effects in `R`\n",
    "As we saw previously, we can form omnibus tests from explicit model comparisons using the `anova()` function. Using this method, the main effect of `origin` would be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "782fc3bb",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of Variance Table\n",
      "\n",
      "Model 1: mpg ~ vs\n",
      "Model 2: mpg ~ origin + vs\n",
      "  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   \n",
      "1     30 629.52                                \n",
      "2     29 496.86  1    132.66 7.7428 0.009386 **\n",
      "---\n",
      "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n"
     ]
    }
   ],
   "source": [
    "null.mod <- lm(mpg ~ vs,          data=mtcars)\n",
    "full.mod <- lm(mpg ~ origin + vs, data=mtcars)\n",
    "\n",
    "print(anova(null.mod,full.mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291846e1",
   "metadata": {},
   "source": [
    "and the main effect of `vs` would be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d482c4de",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of Variance Table\n",
      "\n",
      "Model 1: mpg ~ origin\n",
      "Model 2: mpg ~ origin + vs\n",
      "  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n",
      "1     30 784.06                                  \n",
      "2     29 496.86  1     287.2 16.763 0.0003096 ***\n",
      "---\n",
      "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n"
     ]
    }
   ],
   "source": [
    "null.mod <- lm(mpg ~ origin,      data=mtcars)\n",
    "full.mod <- lm(mpg ~ origin + vs, data=mtcars)\n",
    "\n",
    "print(anova(null.mod,full.mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054947b2",
   "metadata": {},
   "source": [
    "Or, we could simply give the full model to the `Anova()` function from the `car` package and the model comparisons would be conducted automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "974f11f2",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anova Table (Type II tests)\n",
      "\n",
      "Response: mpg\n",
      "          Sum Sq Df F value    Pr(>F)    \n",
      "origin    132.66  1  7.7428 0.0093864 ** \n",
      "vs        287.20  1 16.7628 0.0003096 ***\n",
      "Residuals 496.86 29                      \n",
      "---\n",
      "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n"
     ]
    }
   ],
   "source": [
    "library(car)\n",
    "print(Anova(full.mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c051e2",
   "metadata": {},
   "source": [
    "Compare the `Anova()` output with the outputs from the model comparisons to reassure yourself that these are *identical*. Importantly, notice that generating the full ANOVA table using `anova()` gives us a *different* answer for the effect of `origin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cdcf6cfb",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of Variance Table\n",
      "\n",
      "Response: mpg\n",
      "          Df Sum Sq Mean Sq F value    Pr(>F)    \n",
      "origin     1 341.99  341.99  19.961 0.0001110 ***\n",
      "vs         1 287.20  287.20  16.763 0.0003096 ***\n",
      "Residuals 29 496.86   17.13                      \n",
      "---\n",
      "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n"
     ]
    }
   ],
   "source": [
    "print(anova(full.mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3fbf01",
   "metadata": {},
   "source": [
    "We will discuss the reasons why in the final lesson of this unit. For now, this should hopefully convince you to always use `Anova()` instead of `anova()`, as the results are not always equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1f3651",
   "metadata": {},
   "source": [
    "## The Full Factorial Model\n",
    "So far, we have looked at the additive model that assumes a constant row and column difference between the cell means. However, as alluded to earlier, this additive assumption may not always hold. Indeed, in many real-world examples, we *expect* the factors in the model to influence each other. A good example of this would be a basic $2 \\times 2$ model of a clinical trial. If Factor A is *diagnosis* (*patient*, *control*) and Factor B is *drug* (*placebo*, *medication*), then the expectation might be that the effect of the drug (i.e. *medication vs placebo*) is different depending upon whether you are a *control* or a *patient*. For instance, if the drug is very targeted then there may only be a difference between the medication and a placebo if you are patient, but not if you are a control. In this example, the effect of *drug* depends upon *diagnosis*. When *diagnosis = control* there is no effect, but when *diagnosis = patient* there is an effect. This form of dependence between the levels of multiple factors is known as an *interaction*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce5e521",
   "metadata": {},
   "source": [
    "### Interactions as Multiplicative Effects\n",
    "So, given what we have described above, how do we accommodate an interaction into the additive model?\n",
    "\n",
    "\n",
    "So, the interaction terms represents that \"bit extra\" that we need in order to allow the fitted means to be the same as the sample means. The additive model only gets us so far. The interaction terms allows for a cell-specific adjustment to shift the additive model means so that they are the actual cell means. The larger this adjustment, the larger the interaction effect. In other words, the more that effect of one factor depends upon the levels of the other. \n",
    "\n",
    "Note that you do not *have* to include an interaction. A typical approach in Psychology is always to specify a *full factorial* model containing every main effect and every possible interaction. In general, this is probably the approach you should take, otherwise you make strong assumptions that all interactions are 0. However, there is nothing that says you *have* to do this. You have the flexibility to only include the effects that you want in the model. You just have to be aware of the consequences of doing so. \n",
    "\n",
    "$$\n",
    "SS_{\\text{model}} = SS_{A} + SS_{B} + SS_{AB}\n",
    "$$\n",
    "\n",
    "### Cell Means and Marginal Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fe63c1",
   "metadata": {},
   "source": [
    "\n",
    "```{admonition} Interaction notation\n",
    ":class: tip\n",
    "There are different conventions for writing an interaction into a model. Some authors like to add an additional symbol to denote the interaction, leaving the subscripts to imply that the term is an interaction rather than a main effect. For instance\n",
    "\n",
    "$$\n",
    "y_{ijk} = \\mu +\\alpha_{i} + \\beta_{j} + \\gamma_{ij} + \\epsilon_{ijk},\n",
    "$$\n",
    "\n",
    "where $\\gamma_{ij}$ is the interaction. Personally, we like to use the following notation as it makes the interaction terms much more explicit, particularly in terms of which effects the interaction corresponds to\n",
    "\n",
    "$$\n",
    "y_{ijk} = \\mu +\\alpha_{i} + \\beta_{j} + (\\alpha\\beta)_{ij} + \\epsilon_{ijk}.\n",
    "$$\n",
    "\n",
    "For a model containing multiple interactions, this helps to make the meaning of the terms clearer, rather than somewhat hiding it in the subscripts. For instance, a 3-way full factorial ANOVA would be\n",
    "\n",
    "$$\n",
    "y_{ijkl} = \\mu +\\alpha_{i} + \\beta_{j} + \\gamma_{k} + (\\alpha\\beta)_{ij} + (\\beta\\gamma)_{jk} + (\\alpha\\gamma)_{ik} + (\\alpha\\beta\\gamma)_{ijk} + \\epsilon_{ijkl},\n",
    "$$\n",
    "\n",
    "where each main effect, 2-way interaction and 3-way interaction is clearly denoted.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10cbb17",
   "metadata": {},
   "source": [
    "\n",
    "### Interactions as Multiplicative Effects\n",
    "\n",
    "### Interpreting Main Effects in the Presence of an Interaction\n",
    "... We can see that averaging over either the rows or columns of the means table only makes sense when additivity is assumed. If we do this when there is not a constant row or column effect, the very meaning of the main effect breaks down. If we want to think of a main effect as the consistent effect of a factor irrespective of other factors, this is no longer valid when those effects are *not* consistent. As such, main effects must assume additivity to make any sense. If there is a large interaction effect, the very concept of a main effects no longer make sense. \n",
    "\n",
    "Indeed, you do not need to know anything about the model to see this. The interaction tells us that the main effect depends upon the level of another factor. Why would we then try to look at a main effect *ignoring* that factor? We know that other factor matters. That is what the interaction tells us. Unfortuantely, it is common practise in Psychology to ignore this and try to interpret main effects in the presence of significant interactions. Hopefully it is clear how meaningless this actually is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdf905e",
   "metadata": {},
   "source": [
    "[^engine-foot]: Not that it really matters for understanding an ANOVA, but a straight engine has cylinders in a straight line, while a V-shaped engine has two banks of cylinders arranged in a \"V\". V-shaped engines tend to be more compact and powerful. Straight engines are often more balanced, whereas V engines are shorter and can be better for performance-focused applications.\n",
    "\n",
    "[^unname-foot]: All the `unname()` function is doing here is preventing `R` from printing any labels alongside these results and making them less clear. Sometimes, values carry labels with them across different calculations and you get some odd names, like `(Intercept)` being printed above the calculations. This stops that from happening. You can remove it, if you like, and see what happens.\n",
    "\n",
    "[^reganova-foot]: If we want to remove *everything*, we can always use the regression ANOVA report at the bottom of the summary table, as mentioned at the end of the previous part of the lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043cb46e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
